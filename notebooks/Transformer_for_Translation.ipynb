{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMNG7kmmpx5sfjTixoNeip"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Transformer Model for Language Translation\n",
        "\n",
        "- Build a translation model from scratch using PyTorch:\n",
        "    - Preprocess textual data\n",
        "    - Design the transformer architecture\n",
        "    - Train the model using parallel computing\n",
        "    - Evaluate the model performance\n",
        "    - Generate translation\n",
        "- Translate a PDF document from German to English"
      ],
      "metadata": {
        "id": "4nLQeHNnZ3HA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install required libraries"
      ],
      "metadata": {
        "id": "K_Ghe2eXaf0x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXaU3y9uZtvC",
        "outputId": "0abb6609-bf35-45eb-cae1-96ef7678261e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.6.1 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting de-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from de-core-news-sm==3.7.0) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2025.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.1)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.1)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "# Install compatible core libraries (Torch, TorchText, TorchData)\n",
        "!pip install -Uqq torch==2.0.1 torchtext==0.15.2 torchdata==0.6.1\n",
        "\n",
        "# ğŸ”¤ NLP + Utility Libraries\n",
        "!pip install -Uqq portalocker==2.7.0\n",
        "!pip install -Uqq nltk==3.8.1\n",
        "!pip install -Uqq spacy==3.7.2\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# ğŸ“„ PDF Utilities\n",
        "!pip install -Uqq pdfplumber==0.9.0\n",
        "!pip install -Uqq fpdf==1.7.2\n",
        "\n",
        "# Download required files\n",
        "!wget -q 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py'\n",
        "!wget -q 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer.pt'\n",
        "!wget -q 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/input_de.pdf'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Required Libraries"
      ],
      "metadata": {
        "id": "0UogdmJca6Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import math\n",
        "import warnings\n",
        "from typing import Iterable, List\n",
        "\n",
        "# Third-party imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.nn import Transformer\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Specialized imports with error handling\n",
        "try:\n",
        "    from torchtext.datasets import multi30k, Multi30k\n",
        "    TORCHTEXT_AVAILABLE = True\n",
        "    print(\"TorchText datasets imported successfully\")\n",
        "except (OSError, ImportError) as e:\n",
        "    print(f\"TorchText import failed: {e}\")\n",
        "    print(\"Using alternative dataset loading from downloaded file\")\n",
        "    TORCHTEXT_AVAILABLE = False\n",
        "\n",
        "    # Import the alternative dataloader\n",
        "    try:\n",
        "        from Multi30K_de_en_dataloader import *\n",
        "        print(\"Alternative Multi30K dataloader imported successfully\")\n",
        "    except ImportError:\n",
        "        print(\"Alternative dataloader not found. Make sure Multi30K_de_en_dataloader.py is downloaded\")\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Warning suppression\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "CIdiUlCXa_8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79ecba4b-e938-4abb-f736-615f64d43fae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TorchText datasets imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define Special Symbols and Indices\n",
        "\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
      ],
      "metadata": {
        "id": "u3AoFUiVbs3G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader\n",
        "\n",
        "In the English-German Multi30K dataset, first load the data and break down sentences into words or smaller pieces, called tokens. From these tokens, create a unique list or vocabulary. Each token is then turned into a specific number using this vocabulary. Because sentences can be of different lengths,  add padding to make them all the same size in a batch. All this processed data is then organized into a PyTorch DataLoader, making it easy to use for training neural networks."
      ],
      "metadata": {
        "id": "F631ZqPab_d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run Multi30K_de_en_dataloader.py"
      ],
      "metadata": {
        "id": "DvqFbyxAb-Xp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Given the exploratory work, use a batch size of one\n",
        "\n",
        "train_dataloader, _ = get_translation_dataloaders(batch_size = 1)"
      ],
      "metadata": {
        "id": "MiRW9P4nglBp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To obtain diverse examples, we can cycle through multiple samples\n",
        "# since the dataset is sorted by length.\n",
        "\n",
        "data_itr=iter(train_dataloader)\n",
        "\n",
        "for n in range(1000):\n",
        "    german, english= next(data_itr)"
      ],
      "metadata": {
        "id": "4bxaK3YWg08l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset is structured as sequence-batch-feature,\n",
        "# rather than the typical batch-feature-sequence.\n",
        "# For compatibility with our utility functions, we can transpose the dataset.\n",
        "\n",
        "german=german.T\n",
        "english=english.T"
      ],
      "metadata": {
        "id": "pW6SmbFOhAgL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print out the text by converting the indexes to words using ```index_to_german``` and ```index_to_english```"
      ],
      "metadata": {
        "id": "tOh0U8ibhoGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n in range(5):\n",
        "    german, english= next(data_itr)\n",
        "\n",
        "    print(\"sample {}\".format(n))\n",
        "    print(\"german input\")\n",
        "    print(index_to_german(german))\n",
        "    print(\"english target\")\n",
        "    print(index_to_eng(english))\n",
        "    print(\"_________\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUFVLqa-hrhL",
        "outputId": "b0692465-33f1-4f0c-a93a-10218765b4d7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample 0\n",
            "german input\n",
            "<bos> Ein FeuerwehrangehÃ¶riger arbeitet bei einem Brand . <eos>\n",
            "english target\n",
            "<bos> A firefighter is working at a fire . <eos>\n",
            "_________\n",
            "\n",
            "sample 1\n",
            "german input\n",
            "<bos> Ein Mann spielt auf einem FlÃ¼gel . <eos>\n",
            "english target\n",
            "<bos> A man playing a black grand piano . <eos>\n",
            "_________\n",
            "\n",
            "sample 2\n",
            "german input\n",
            "<bos> Ein brauner Hund spielt im Schnee . <eos>\n",
            "english target\n",
            "<bos> A brown dog plays in the snow . <eos>\n",
            "_________\n",
            "\n",
            "sample 3\n",
            "german input\n",
            "<bos> Mehrere Hunde in einem winterlichen Ambiente . <eos>\n",
            "english target\n",
            "<bos> Several dogs grouped together in a winter setting . <eos>\n",
            "_________\n",
            "\n",
            "sample 4\n",
            "german input\n",
            "<bos> Ein Mann klettert einen Felsen hoch . <eos>\n",
            "english target\n",
            "<bos> A man climbs up a rock . <eos>\n",
            "_________\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if a GPU is available and use it; otherwise, you'll use the CPU.\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzZg61iyiC9e",
        "outputId": "50e5bbdc-05c7-4ccc-c58b-40783b396b83"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare"
      ],
      "metadata": {
        "id": "7sQ47CRqi400"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masking\n",
        "\n",
        "During training, the entire sequence is visible to the model and used as input to learn patterns. In contrast, for prediction, the future sequence is not available. To do this, employ masking to simulate this lack of future data, ensuring the model learns to predict without seeing the actual next tokens. It is crucial for ensuring certain positions are not attended to. The function ```generate_square_subsequent_mask``` produces an upper triangular matrix, which ensures that during decoding, a token can't attend to future tokens."
      ],
      "metadata": {
        "id": "JonwMIQ_i_yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz,device=DEVICE):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask"
      ],
      "metadata": {
        "id": "NHCwpBs8i8H_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ```create_mask``` function, on the other hand, generates both source and target masks, as well as padding masks based on the provided source and target sequences. The padding masks ensure that the model doesn't attend to pad tokens, providing a streamlined attention."
      ],
      "metadata": {
        "id": "RuZJkIS_jO7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(src, tgt,device=DEVICE):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "id": "7vNJ-zRijRJN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional encoding\n",
        "The transformer model doesn't have built-in knowledge of the order of tokens in the sequence. To give the model this information, positional encodings are added to the tokens embeddings. These encodings have a fixed pattern based on their position in the sequence.\n"
      ],
      "metadata": {
        "id": "gGrdBcv4jWMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add positional information to the input tokens\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
      ],
      "metadata": {
        "id": "26E1zirmjUj4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token embedding\n",
        "Token embedding, also known as word embedding or word representation, is a way to convert words or tokens from a text corpus into numerical vectors in a continuous vector space. Each unique word or token in the corpus is assigned a fixed-length vector where the numerical values represent various linguistic properties of the word, such as its meaning, context, or relationships with other words.\n",
        "\n",
        "The `TokenEmbedding` class converts numerical tokens into embeddings."
      ],
      "metadata": {
        "id": "VnHHnd6mjdXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ],
      "metadata": {
        "id": "nvS6jPLEjlaJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer architecture for language translation\n",
        "This process relies on an encoder-decoder architecture.\n",
        "\n",
        "### Tokenization and positional encoding\n",
        "The source language text (the input sequence) is tokenized, which means it's divided into individual words or subwords. These tokens are then converted into numerical representations. To preserve word order information, positional encodings are added to these numerical tokens.\n",
        "\n",
        "### Encoder processing\n",
        "Next pass these numerical tokens through the encoder. The encoder is composed of multiple layers, each containing self-attention mechanisms and feed-forward neural networks. This architecture allows the transformer model to process the entire input sequence at once, in contrast to traditional RNN-based models like LSTMs or GRUs, which process input sequentially.\n",
        "\n",
        "### Decoding with teacher forcing\n",
        "During training, the target language text (the correct output sequence) is also tokenized and converted into numerical tokens. \"Teacher forcing\" is a training technique where the decoder is provided with the target tokens as input. The decoder uses both the encoder's output and the previously generated tokens (starting with a special start-of-sequence token) to predict the next token in the sequence.\n",
        "\n",
        "### Output generation and loss calculation\n",
        "The decoder generates the translated sequence token by token. At each step, the decoder predicts the next token in the target sequence. The predicted sequence from the decoder is then compared to the actual target sequence using a loss function, typically cross-entropy loss for translation tasks. This loss function quantifies how well the model's predictions match the true target sequence.\n",
        "\n",
        "## Seq2SeqTransformer\n",
        "Represents the core of the transformer model for language translation.\n",
        "\n",
        "- **Data loading:** Loading and preparing the training data, which includes source language text and corresponding target language text.\n",
        "\n",
        "- **Model initialization:** Initializing the transformer model, including setting up the encoder, decoder, positional encodings, and other necessary components.\n",
        "\n",
        "- **Optimizer setup:** Choosing an appropriate optimizer, such as Adam, and defining learning rate schedules to update model parameters during training.\n",
        "\n",
        "- **Training loop:** Iterating through the training data for multiple epochs, using teacher forcing to guide the model's learning process.\n",
        "\n",
        "- **Loss monitoring:** Recording and potentially plotting training losses for each epoch. These losses indicate how well the model is learning to perform language translation."
      ],
      "metadata": {
        "id": "RfzKL7abkWSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        outs =outs.to(DEVICE)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ],
      "metadata": {
        "id": "00VqMGSplYJa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "\n",
        "The diagram below illustrates the sequence prediction or inference process. You can begin by feeding the indices of your desired translation sequence into the encoder, represented by the lower-left orange section. The resulting embeddings from the encoder are then channeled into the decoder, highlighted in green. Alongside, a start token is introduced at the beginning of the decoder input, as depicted at the base of the green segment.\n",
        "\n",
        "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/predict_transformers.png\" alt=\"transformer\" width=\"60%\">\n",
        "\n",
        "The decoder's output is then mapped onto a vocabulary-sized vector using a linear layer. Following this, a softmax function converts these vector scores into probabilities. The highest probability, as determined by the argmax function, provides the index of your predicted word within the translated sequence. This predicted index is fed back into the decoder in conjunction with the initial sequence, setting the stage to determine the subsequent word in the translation. This autoregressive process is demonstrated by the arrow pointing to form the top of the decoder, in green, to the bottom.\n"
      ],
      "metadata": {
        "id": "swmz24BRmIEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)"
      ],
      "metadata": {
        "id": "-WWGrOa4mOG6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Load the weights of apretrained transformer model from the file 'transformer.pt'.\n",
        "\n",
        " transformer.load_state_dict(torch.load('transformer.pt', map_location=DEVICE, ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdyAafEinAW1",
        "outputId": "5a4aeffc-02cd-4ed5-844a-ca3580191f51"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in range(100):\n",
        "    src ,tgt= next(data_itr)\n",
        "\n",
        "print(\"English target\",index_to_eng(tgt))\n",
        "print(\"German input\",index_to_german(src))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18tS3J_GnQNd",
        "outputId": "a453501c-ac58-435f-b643-cd300e0135c3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English target <bos> A construction worker is talking on the phone in a train tunnel . <eos>\n",
            "German input <bos> Ein Bauarbeiter telefoniert in einem Bahntunnel . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = src.shape[0]\n",
        "num_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6_A9gnfnexr",
        "outputId": "94baa060-dce2-4e66-fc91-18b24cf2f9f2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool).to(DEVICE )\n",
        "src_mask[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6DKtboJnjG-",
        "outputId": "08269538-ed88-44da-8d2c-249b225a1ecc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False, False, False, False, False, False, False, False, False],\n",
              "        [False, False, False, False, False, False, False, False, False],\n",
              "        [False, False, False, False, False, False, False, False, False],\n",
              "        [False, False, False, False, False, False, False, False, False],\n",
              "        [False, False, False, False, False, False, False, False, False],\n",
              "        [False, False, False, False, False, False, False, False, False],\n",
              "        [False, False, False, False, False, False, False, False, False],\n",
              "        [False, False, False, False, False, False, False, False, False],\n",
              "        [False, False, False, False, False, False, False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_=src[:,0].unsqueeze(1)\n",
        "print(src_.shape)\n",
        "print(src.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8XgVuLCnncj",
        "outputId": "5b2e3001-a2a1-45c5-dfbe-b489180c98b7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([9, 1])\n",
            "torch.Size([9, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = transformer.encode(src_, src_mask)\n",
        "memory.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZbVz2CNntYX",
        "outputId": "55262157-e4a0-40a4-a74d-2810413baa28"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([9, 1, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ys = torch.ones(1, 1).fill_(BOS_IDX).type(torch.long).to(DEVICE)\n",
        "ys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k86v4tejnyzJ",
        "outputId": "d6358f6b-cf8a-4466-b15c-22b581973718"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)\n",
        "tgt_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lyyrrnq_n3XE",
        "outputId": "9fe14620-4784-4878-ccd6-236570f52e16"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = transformer.decode(ys, memory, tgt_mask)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUyBzVhCn6BN",
        "outputId": "057c43e2-79b2-4635-9354-90ca71146fee"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logit = transformer.generator(out[:, -1])\n",
        "logit.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJVWIfONoCK6",
        "outputId": "2d0f273f-3f02-41d8-9bd2-693d46236d28"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10837])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  _, next_word_index = torch.max(logit, dim=1)\n",
        "  print(\"engish output:\",index_to_eng(next_word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE4lGngsw5eB",
        "outputId": "2772cab6-08e1-41af-9661-8240eda7c026"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "engish output: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_word_index=next_word_index.item()\n",
        "next_word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmIL60NJxObO",
        "outputId": "ca78e0c5-2036-4518-ea90-9408c4525aec"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ys = torch.cat([ys,torch.ones(1, 1).type_as(src.data).fill_(next_word_index)], dim=0)\n",
        "ys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfCeCDGY7aqn",
        "outputId": "b1eb5af8-96b5-48d0-ee15-2918bd869d67"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2],\n",
              "        [6]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the target mask for the current sequence length.\n",
        "\n",
        "tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)\n",
        "tgt_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p2ZtZul7bvL",
        "outputId": "0cecb110-e99a-43e2-8791-a9c2d1fa6974"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False,  True],\n",
              "        [False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the current sequence using the transformer and retrieve the output.\n",
        "\n",
        "out = transformer.decode(ys, memory, tgt_mask)\n",
        "out = out.transpose(0, 1)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gK6-lox7hJc",
        "outputId": "ce067a33-e3dd-4288-a8dd-814bdfbcdaf3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 2, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the word probabilities for the last predicted word.\n",
        "prob = transformer.generator(out[:, -1])\n",
        "\n",
        "# Find the word index with the highest probability.\n",
        "_, next_word_index = torch.max(prob, dim=1)\n",
        "\n",
        "# Print the predicted English word.\n",
        "print(\"English output:\", index_to_eng(next_word_index))\n",
        "\n",
        "# Convert the tensor value to a Python scalar.\n",
        "next_word_index = next_word_index.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK3I2HYc7mJY",
        "outputId": "e11cd497-4e46-4d46-c1bd-46f1d05f2d6f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English output: construction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ys = torch.cat([ys,torch.ones(1, 1).type_as(src.data).fill_(next_word_index)], dim=0)\n",
        "print(\"English output:\",index_to_eng(ys))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kM3cg07R7riu",
        "outputId": "4aedf083-17e0-4917-b768-7f6e5b40b466"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English output: <bos> A construction construction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Starting with the initial output of the encoder and the token, the decoder's output is looped back into the decoder until the translated sequence is fully decoded. This cycle continues until the length of the new translated sequence matches that of the original sequence."
      ],
      "metadata": {
        "id": "vBtFaabS792f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys"
      ],
      "metadata": {
        "id": "XUZuwUFU7-sk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src\n",
        "src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool).to(DEVICE )"
      ],
      "metadata": {
        "id": "F4eUaVcc8ETO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=src.shape[0]+5\n",
        "max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dpliy-iv8Il-",
        "outputId": "9153a3c6-80b8-40f6-fdba-57cde4dba56f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ys=greedy_decode(transformer, src, src_mask, max_len, start_symbol=BOS_IDX)\n",
        "print(\"English  (ys):\",index_to_eng(ys))\n",
        "print(\"English (tgt):\",index_to_eng(tgt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXWa8oXh8LXc",
        "outputId": "e6434db2-0aca-4f86-d196-e6b20f0fdca5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English  (ys): <bos> A construction worker is talking on a train in a train train .\n",
            "English (tgt): <bos> A construction worker is talking on the phone in a train tunnel . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoding the differences: Training vs. inference in neural machine translation\n",
        "\n",
        "During the inference phase, when the model is deployed for actual translation tasks, the decoder generates the sequence without access to the expected target sequence. Instead, it bases its predictions on the encoder's output and the tokens it has produced in sequence so far. The process is autoregressive, with the decoder continually predicting the next token until it outputs an end-of-sequence token, indicating the translation is complete.\n",
        "\n",
        "The key difference between the training and inference stages lies in the inputs to the decoder. During training, the decoder benefits from exposure to the ground truth--receiving the exact target sequence tokens incrementally through a technique known as \"teacher forcing.\" This approach is in stark contrast to some other neural network architectures that rely on the network's previous predictions as inputs during training. Once training concludes, the datasets used resemble those employed in more conventional neural network models, providing a familiar foundation for comparison and evaluation.\n",
        "\n",
        "First, import `CrossEntropyLoss` loss and create a Cross Entropy Loss object The loss will  not be calculated when the token with index `PAD_IDX` an input."
      ],
      "metadata": {
        "id": "2j9Xi9lr8v1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "loss_fn = CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "metadata": {
        "id": "RlIjN0JC8snp"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tgt_input = tgt[:-1, :]\n",
        "print(index_to_eng(tgt_input))\n",
        "print(index_to_eng(tgt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2STfJLh83aT",
        "outputId": "7446d999-312b-4475-c789-231f7132b3a9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos> A construction worker is talking on the phone in a train tunnel .\n",
            "<bos> A construction worker is talking on the phone in a train tunnel . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the required masks\n",
        "\n",
        "src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "print(f\"Shape of src_mask: {src_mask.shape}\")\n",
        "print(f\"Shape of tgt_mask: {tgt_mask.shape}\")\n",
        "print(f\"Shape of src_padding_mask: {src_padding_mask.shape}\")\n",
        "print(f\"Shape of tgt_padding_mask: {tgt_padding_mask.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUqIEiRI9AWT",
        "outputId": "43cc82bb-6094-43d1-9229-516de5783d97"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of src_mask: torch.Size([9, 9])\n",
            "Shape of tgt_mask: torch.Size([14, 14])\n",
            "Shape of src_padding_mask: torch.Size([1, 9])\n",
            "Shape of tgt_padding_mask: torch.Size([1, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_padding_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbmow9t19IVG",
        "outputId": "911e0e4a-ca49-47fb-c4a7-439311a5f818"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False, False, False, False, False, False, False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tgt_mask)\n",
        "[index_to_eng( tgt_input[t==0])  for t in tgt_mask] #index_to_eng(tgt_input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpCbDLGw9LHX",
        "outputId": "1b3bb670-f905-45d3-c799-f1e365899807"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<bos>',\n",
              " '<bos> A',\n",
              " '<bos> A construction',\n",
              " '<bos> A construction worker',\n",
              " '<bos> A construction worker is',\n",
              " '<bos> A construction worker is talking',\n",
              " '<bos> A construction worker is talking on',\n",
              " '<bos> A construction worker is talking on the',\n",
              " '<bos> A construction worker is talking on the phone',\n",
              " '<bos> A construction worker is talking on the phone in',\n",
              " '<bos> A construction worker is talking on the phone in a',\n",
              " '<bos> A construction worker is talking on the phone in a train',\n",
              " '<bos> A construction worker is talking on the phone in a train tunnel',\n",
              " '<bos> A construction worker is talking on the phone in a train tunnel .']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we call `model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)`,  the forward method of the `Seq2SeqTransformer` class. This process generates logits for the target sequence, which can then be translated into actual tokens by taking the highest probability prediction at each step in the sequence."
      ],
      "metadata": {
        "id": "5rGxdm9M9QeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss"
      ],
      "metadata": {
        "id": "Lm1n86Qs9T_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = transformer(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "print(\"output shape\",logits.shape)\n",
        "print(\"target shape\",tgt_input.shape)\n",
        "print(\"source shape \",src.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9zCyi5Y9Wam",
        "outputId": "2489da28-8ece-4ee8-c7ef-9374fad6beb7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output shape torch.Size([14, 1, 10837])\n",
            "target shape torch.Size([14, 1])\n",
            "source shape  torch.Size([9, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tgt_out = tgt[1:, :]\n",
        "print(tgt_out.shape)\n",
        "[index_to_eng(t)  for t in tgt_out]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYwQXA3S9fg4",
        "outputId": "bcabab26-7702-4dc7-b379-527960288035"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([14, 1])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'construction',\n",
              " 'worker',\n",
              " 'is',\n",
              " 'talking',\n",
              " 'on',\n",
              " 'the',\n",
              " 'phone',\n",
              " 'in',\n",
              " 'a',\n",
              " 'train',\n",
              " 'tunnel',\n",
              " '.',\n",
              " '<eos>']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tgt_out_flattened = tgt_out.reshape(-1)\n",
        "print(tgt_out_flattened.shape)\n",
        "tgt_out_flattened"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6wo4DoE9jEM",
        "outputId": "03f2772f-094b-4cce-feda-4c4e2d0db45d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([14])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  6, 264, 348,  10, 121,   9,   8, 360,   7,   4, 240, 825,   5,   3])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[\"input: {} target: {}\".format(index_to_eng( tgt_input[m==0]),index_to_eng( t))  for m,t in zip(tgt_mask,tgt_out)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbApEU7j9m0o",
        "outputId": "c6428664-6b5b-4a7f-f485-ba97814d8e54"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['input: <bos> target: A',\n",
              " 'input: <bos> A target: construction',\n",
              " 'input: <bos> A construction target: worker',\n",
              " 'input: <bos> A construction worker target: is',\n",
              " 'input: <bos> A construction worker is target: talking',\n",
              " 'input: <bos> A construction worker is talking target: on',\n",
              " 'input: <bos> A construction worker is talking on target: the',\n",
              " 'input: <bos> A construction worker is talking on the target: phone',\n",
              " 'input: <bos> A construction worker is talking on the phone target: in',\n",
              " 'input: <bos> A construction worker is talking on the phone in target: a',\n",
              " 'input: <bos> A construction worker is talking on the phone in a target: train',\n",
              " 'input: <bos> A construction worker is talking on the phone in a train target: tunnel',\n",
              " 'input: <bos> A construction worker is talking on the phone in a train tunnel target: .',\n",
              " 'input: <bos> A construction worker is talking on the phone in a train tunnel . target: <eos>']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, calculate the loss as the output from the transformer's decoder is provided as input to the cross-entropy loss function along with the target sequence values. Given that the transformer's output has the dimensions sequence length, batch size, and features (vocab_size), it's necessary to reshape this output to align with the standard input format required by the cross-entropy loss function. This step ensures that the loss is calculated correctly, comparing the predicted sequence against the ground truth at each time step across the batch using the reshape method"
      ],
      "metadata": {
        "id": "45VQLPCh9u9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jHudK6Y9wnQ",
        "outputId": "510f2b51-7a64-480e-943f-b04209aad10b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4338, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate\n",
        "By following these procedures, we can develop a function that is capable of making predictions and subsequently computing the corresponding loss on the validation data, you will use this function later on."
      ],
      "metadata": {
        "id": "c1XQ9sJS96pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(val_dataloader))"
      ],
      "metadata": {
        "id": "GT7mTIKj-BJg"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model\n",
        "Incorporating the previously steps, proceed to train the model. Apart from these specific procedures, the overall training process conforms to the conventional methods employed in neural network training."
      ],
      "metadata": {
        "id": "6OYDbjVz-GaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, optimizer, train_dataloader):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "\n",
        "    # Wrap train_dataloader with tqdm for progress logging\n",
        "    train_iterator = tqdm(train_dataloader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for src, tgt in train_iterator:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "        src_mask = src_mask.to(DEVICE)\n",
        "        tgt_mask = tgt_mask.to(DEVICE)\n",
        "        src_padding_mask = src_padding_mask.to(DEVICE)\n",
        "        tgt_padding_mask = tgt_padding_mask.to(DEVICE)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "        logits = logits.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "        # Update tqdm progress bar with the current loss\n",
        "        train_iterator.set_postfix(loss=loss.item())\n",
        "\n",
        "    return losses / len(list(train_dataloader))"
      ],
      "metadata": {
        "id": "y7uGuQbx-F2v"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3"
      ],
      "metadata": {
        "id": "Ie3GYBHn-PuV"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, val_dataloader = get_translation_dataloaders(batch_size = BATCH_SIZE)"
      ],
      "metadata": {
        "id": "NPeoLdWk-V04"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "transformer = transformer.to(DEVICE)"
      ],
      "metadata": {
        "id": "PxoUZEa6-ZSX"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "metadata": {
        "id": "6VnInRP5-cbP"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the train loss and validation loss list.\n",
        "\n",
        "TrainLoss=[]\n",
        "ValLoss=[]"
      ],
      "metadata": {
        "id": "09Vi2txy-e4t"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer, train_dataloader)\n",
        "    TrainLoss.append(train_loss)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    ValLoss.append(val_loss)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "torch.save(transformer.state_dict(), 'transformer_de_to_en_model.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "YpkAZSZ6-l-X",
        "outputId": "cd74b9f3-8a8a-42a9-9fb9-2795928fbdb0"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 5.076, Val loss: 4.376, Epoch time = 1766.181s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-59-3026763981.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mTrainLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-53-1496351917.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, train_dataloader)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtgt_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss for the training and validation data.\n",
        "\n",
        "epochs = range(1, len(TrainLoss) + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, TrainLoss, 'r', label='Training loss')\n",
        "plt.plot(epochs,ValLoss, 'b', label='Validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "YDLDKqTX-saD",
        "outputId": "690be4ed-bb24-4e88-8281-b72cf243660e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the saved model\n",
        "\n",
        "We can load the pretrained model that is provided."
      ],
      "metadata": {
        "id": "srzIUzNu-2L8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer_de_to_en_model.pt'\n",
        "transformer.load_state_dict(torch.load('transformer_de_to_en_model.pt',map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSbu9ub0_ACa",
        "outputId": "dba9d3e1-e04b-4e4e-8b43-b09346094266"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-09 12:07:50--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer_de_to_en_model.pt\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.45.118.108\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 144561810 (138M) [binary/octet-stream]\n",
            "Saving to: â€˜transformer_de_to_en_model.ptâ€™\n",
            "\n",
            "transformer_de_to_e 100%[===================>] 137.86M  38.9MB/s    in 4.2s    \n",
            "\n",
            "2025-07-09 12:07:55 (33.0 MB/s) - â€˜transformer_de_to_en_model.ptâ€™ saved [144561810/144561810]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translation and evaluation\n",
        "\n",
        "Using the greedy_decode function that we defined earlier, we can create a translator function that generates English translation of an input German text."
      ],
      "metadata": {
        "id": "Lsy06Av-_Fb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# translate input sentence into target language\n",
        "\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "metadata": {
        "id": "Nun09mhu-wEp"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n in range(5):\n",
        "    german, english= next(data_itr)\n",
        "\n",
        "    print(\"German Sentence:\",index_to_german(german).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
        "    print(\"English Translation:\",index_to_eng(english).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
        "    print(\"Model Translation:\",translate(transformer,index_to_german(german)))\n",
        "    print(\"_________\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShrZLcSf_Nyt",
        "outputId": "efe5fc7f-06c0-4d1b-d631-2aee9163191c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "German Sentence:  Zwei Bauarbeiter arbeiten auf einem GerÃ¼st . \n",
            "English Translation:  Two construction workers are working on a scaffold . \n",
            "Model Translation:  Two construction workers are working on a concrete concrete concrete concrete concrete concrete concrete concrete concrete concrete concrete concrete\n",
            "_________\n",
            "\n",
            "German Sentence:  Ein Mann steht auf den Gleisen . \n",
            "English Translation:  A man standing on the tracks . \n",
            "Model Translation:  A man stands on the concrete concrete concrete concrete area on the city skyline . \n",
            "_________\n",
            "\n",
            "German Sentence:  Bauarbeiter reparieren die WÃ¤nde einer U-Bahn . \n",
            "English Translation:  Construction workers repair walls of a subway . \n",
            "Model Translation:  Construction workers repair concrete concrete concrete concrete concrete concrete concrete concrete concrete concrete concrete concrete concrete concrete concrete concrete\n",
            "_________\n",
            "\n",
            "German Sentence:  Bauarbeiter bringen Verkleidungen im Tunnelinneren an . \n",
            "English Translation:  Construction workers applying siding to the inside of a tunnel . \n",
            "Model Translation:  Construction workers use concrete area to some concrete area in area . \n",
            "_________\n",
            "\n",
            "German Sentence:  Ein Arbeiter liest in einem U-Bahn-Zug . \n",
            "English Translation:  A worker taking a reading on a subway train . \n",
            "Model Translation:  A workman is reading a bunch of water park in a city park area . \n",
            "_________\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation with BLEU score\n",
        "To evaluate the generated translations, a function calculate_bleu_score is introduced. It computes the BLEU score, a common metric for machine translation quality, by comparing the generated translation to reference translations. The BLEU score provides a quantitative measure of translation accuracy.\n",
        "\n",
        "The code also includes an example of calculating the BLEU score for a generated translation."
      ],
      "metadata": {
        "id": "Uq7ZlrbI_VvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu_score(generated_translation, reference_translations):\n",
        "    # convert the generated translations and reference translations into the expected format for sentence_bleu\n",
        "    references = [reference.split() for reference in reference_translations]\n",
        "    hypothesis = generated_translation.split()\n",
        "\n",
        "    # calculate the BLEU score\n",
        "    bleu_score = sentence_bleu(references, hypothesis)\n",
        "\n",
        "    return bleu_score"
      ],
      "metadata": {
        "id": "OtRwVQWm_ZA0"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_translation = translate(transformer,\"Ein brauner Hund spielt im Schnee .\")\n",
        "\n",
        "reference_translations = [\n",
        "    \"A brown dog is playing in the snow .\",\n",
        "    \"A brown dog plays in the snow .\",\n",
        "    \"A brown dog is frolicking in the snow .\",\n",
        "    \"In the snow, a brown dog is playing .\"\n",
        "\n",
        "]\n",
        "\n",
        "bleu_score = calculate_bleu_score(generated_translation, reference_translations)\n",
        "print(\"BLEU Score:\", bleu_score, \"for\",generated_translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0VHE7iE_dhJ",
        "outputId": "c2dd2a0d-962d-4386-8af1-18839c6f2720"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 1.0 for  A brown dog plays in the snow . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translating a document\n",
        "Translate a PDF in German to English, with the sequence-to-sequence transformer model.\n",
        "\n",
        "1. **Define the translation function** `translate_pdf` that takes the following parameters:\n",
        "   - `input_file`: The path to the input PDF file to be translated.\n",
        "   - `translator_model`: A model or function that will handle the translation of text.\n",
        "   - `output_file`: The path where the translated PDF will be saved.\n",
        "\n",
        "2. **Read and translate the PDF**:\n",
        "   Use `pdfplumber` to open and read the text from each page of the input PDF. Translate the extracted text using the `translator_model`.\n",
        "\n",
        "3. **Format and write the translated text to a new PDF**:\n",
        "   - Use `textwrap` to wrap the translated text so that it fits within the A4 page width.\n",
        "   - Create a new PDF with `FPDF` and add the wrapped translated text to it.\n",
        "   - Save the new PDF with the translated text to `output_file`."
      ],
      "metadata": {
        "id": "ko5gIvSl_kU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import textwrap\n",
        "from fpdf import FPDF\n",
        "\n",
        "def translate_pdf(input_file, translator_model,output_file):\n",
        "    translated_text = \"\"\n",
        "\n",
        "    # Read the input PDF file\n",
        "    with pdfplumber.open(input_file) as pdf:\n",
        "\n",
        "\n",
        "        # Extract text from each page of the PDF\n",
        "        for page in pdf.pages:\n",
        "            text_content = page.extract_text()\n",
        "            num_pages = len(pdf.pages)\n",
        "            a4_width_mm = 210\n",
        "            pt_to_mm = 0.35\n",
        "            fontsize_pt = 10\n",
        "            fontsize_mm = fontsize_pt * pt_to_mm\n",
        "            margin_bottom_mm = 10\n",
        "            character_width_mm = 7 * pt_to_mm\n",
        "            width_text = a4_width_mm / character_width_mm\n",
        "\n",
        "            pdf = FPDF(orientation='P', unit='mm', format='A4')\n",
        "            pdf.set_auto_page_break(True, margin=margin_bottom_mm)\n",
        "            pdf.add_page()\n",
        "            pdf.set_font(family='Courier', size=fontsize_pt)\n",
        "            # Split the text into sentences\n",
        "            sentences = text_content.split(\".\")\n",
        "\n",
        "            # Translate each sentence using the custom translator model\n",
        "            for sentence in sentences:\n",
        "                translated_sentence = translate(translator_model,sentence)\n",
        "                lines = textwrap.wrap(translated_sentence, width_text)\n",
        "\n",
        "                if len(lines) == 0:\n",
        "                    pdf.ln()\n",
        "\n",
        "                for wrap in lines:\n",
        "                    pdf.cell(0, fontsize_mm, wrap, ln=1)\n",
        "\n",
        "            pdf.output(output_file, 'F')"
      ],
      "metadata": {
        "id": "K-XctZTHAKR6"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEST"
      ],
      "metadata": {
        "id": "AFOUMD5SASdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A document in German\n",
        "\n",
        "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/input_de.pdf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJIVpjG_ALvf",
        "outputId": "5dd9e936-7245-4a5b-f339-07143f493d25"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-09 12:08:20--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/input_de.pdf\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27628 (27K) [application/pdf]\n",
            "Saving to: â€˜input_de.pdf.6â€™\n",
            "\n",
            "input_de.pdf.6      100%[===================>]  26.98K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2025-07-09 12:08:20 (7.02 MB/s) - â€˜input_de.pdf.6â€™ saved [27628/27628]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = \"input_de.pdf\"\n",
        "output_file = 'output_en.pdf'\n",
        "translate_pdf(input_file_path, transformer,output_file)\n",
        "print(\"Translated PDF file is saved as:\", output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJridzXxAYVH",
        "outputId": "2df1ff4e-dbe7-4cfe-b629-a5fbb73aafca"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated PDF file is saved as: output_en.pdf\n"
          ]
        }
      ]
    }
  ]
}