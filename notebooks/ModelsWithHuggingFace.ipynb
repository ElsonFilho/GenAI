{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFfAZNLGtgG3"
   },
   "source": [
    "# **Loading Models and Inference with Hugging Face Inferences**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iufLRqPOy4jl"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qe4M_D5FtSP3",
    "outputId": "02d1a7ee-247c-4331-d0c4-932ee18a134d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n"
     ]
    }
   ],
   "source": [
    "# @title Install Required Libraries\n",
    "\n",
    "!pip install torch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "6UuZIK0tt-Oq"
   },
   "outputs": [],
   "source": [
    "# @title Importing required libraries\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "# You can also use this section to suppress warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zNHlHTDuTWx"
   },
   "source": [
    "# Text classification with DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "05093fb609694955a51abc44d9cd8659",
      "f51b7cefc4e54f519e6016aff6ecdfcb",
      "69f31c68a81546abaa5abd7261f0ee2a",
      "dd15aa6e3db64ad7ba73a58d34bb7e79",
      "43fcf7efb38d4de0a324d7b98ea917fb",
      "924305043dd14f0c91c09c55177e98f1",
      "036be6f362cb412889e5342a9bdd3bf3",
      "4d421df8621041efac204ca31efbaba4",
      "2aec41659bf04078b9db6a8271776554",
      "b919eb2efc8d45f3bebe5e8cd9a30876",
      "5f48c4e3820b48b9890e4cb55a18aead",
      "5b351671fdc549138ff76bccf998f1a2",
      "77bb977e84bb420ab12f97f093b1c7c5",
      "cf3ee53d527149c28b47d007c2eeb054",
      "0bca9c068399475dac813abd7aeabd44",
      "487405a2e2674ceead42bb697c290827",
      "af2cabce2cfa48a4acad6e6127ec71c3",
      "e5cc5378aaf44784b408c3a014135117",
      "bfe5233eb0f9479eb9bbd5712afd5ecd",
      "247b23473338487d9820736adb8fdce3",
      "086f077d61464d72a29f2bc1700fbade",
      "8590e3a6b32e423c9d6e7227e7e0ac0a",
      "a02bad1bb6a449d790509fea8937a145",
      "b96050ad5e4b4e8183bcef32a5cff3dd",
      "08243e0dc075492db6a8c0e641de284a",
      "40753da3a30f48d6873f53113bd2e5d2",
      "6e0bbe76b94941dd8b0c6ca53456084a",
      "a548b9d95db94a8dbc2c38ded23164b3",
      "12a4ec814d3045708bb5b0c88241731e",
      "1df98ca5f9174176aafc0fa733134792",
      "5865cfb93a22400fb8b6ab7f7e0da34b",
      "c3cc575a2ca14a44b75fbf08055ca8bb",
      "cf504956205c4f8bbd9d522efcb0cf37",
      "4b30b236a76e42168ed227ec250d74aa",
      "70a41a8631c44c3cb39ac173cc1abefd",
      "d79932a383fa46e5860a98eba6dd0f66",
      "acc3223e4cd846b4a528f76b46fc0185",
      "332961670a224c2ea0c3c99cc1ac0b35",
      "164a0496090a4bc2a9737ed24a8e97ed",
      "c9bde8c860924592b231f12fe9d0c8ca",
      "c36433cd63cb45fea1ab789dee267ad4",
      "bd295c50e79147baa8a305d1c1cd28f7",
      "ba56534a2e7641efa946ea65fa3fd761",
      "334ffb60bdc24aebb3aa687968e107a0"
     ]
    },
    "id": "lc5R-OC3vYpy",
    "outputId": "f8820aae-ca23-4742-83e1-9b02fa4a558f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05093fb609694955a51abc44d9cd8659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b351671fdc549138ff76bccf998f1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02bad1bb6a449d790509fea8937a145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b30b236a76e42168ed227ec250d74aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Load the tokenizer and model\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EiIEbMNueNo"
   },
   "source": [
    "## Preprocess the input text\n",
    "Tokenize the input text and convert it to a format suitable for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLmENCQ9uWTQ",
    "outputId": "4bd83037-207e-43b6-dd18-b8c44a4a8e48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2419, 22116,  2134,  1005,  1056,  2074,  2377,  2189,  1025,\n",
      "          2027,  2417, 28344,  1996,  2200,  5957,  1997,  2600,  1010,  5132,\n",
      "          1010,  1998,  5154,  1010,  2005,  4726,  1037,  2614,  2008,  3464,\n",
      "          4895, 18900,  7690,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"Led Zeppelin didn't just play music; they redefined the very landscape of rock, blues, and folk, forging a sound that remains unmatched.\"\n",
    "#text =\"One Direction music felt less like artistic expression and more like a carefully engineered product designed for maximum teen idol appeal.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1Ccj9zkvv-n"
   },
   "source": [
    "##  Perform inference\n",
    "The `torch.no_grad()` context manager is used to disable gradient calculation.\n",
    "This reduces memory consumption and speeds up computation, as gradients are not needed for inference (i.e. when you are not training the model). The **inputs syntax is used to unpack a dictionary of keyword arguments in Python. In the context of the model(**inputs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Eo_bGyFKvzRx"
   },
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UHA181SEwQ1E"
   },
   "outputs": [],
   "source": [
    "# Get the logits\n",
    "\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KRV8N_Dvwevu",
    "outputId": "cd24f177-fef2-489f-c231-3363fa40179b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "# Convert logits to probabilities\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = torch.argmax(probs, dim=-1)\n",
    "\n",
    "# Map the predicted class to the label\n",
    "labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "predicted_label = labels[predicted_class]\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tk4hSh-2yzre"
   },
   "source": [
    "# Text generation with GPT-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "a8953658fc654d139a6d1c277e178140",
      "32a75e87611d4dd7bd959b3f6f482071",
      "73a9b16d80cf4eaeadf58deb55376ade",
      "3d9cfecd72594d888207e545939eb9c3",
      "8d35e5827d7340d085bacaefa1f514b1",
      "447b738149854eaeba710da21b9eaffd",
      "7cda45d55aa2430b8cd7900b99b0d12f",
      "78546491d85d42a1945d245e6087ddc2",
      "e671db870be04c2b94f245ab2b02fa39",
      "cc7abeb3c17d4454892bfd6003a232b6",
      "c284c8faf9ac40db8e3163ee495bc6c9",
      "b88c91b162a4470dbeb37abb1e8e7924",
      "7c46757286c54032b1266f04a75013bc",
      "359dc036317f4df283683f79a406a487",
      "1294a26e1b5f4403961411196e911f90",
      "e9a3e99f5ac64167a9a686d46c63cb60",
      "a1a82592f4e043b8ad321e2c20b93943",
      "4cce3c925eb34bb291365f0dc20ce5f0",
      "3f034e0d743642cbaca00fac5020e6e8",
      "8f80436927764116a03995a33b9031a0",
      "918d5210b97541ebbb6133a9964ee783",
      "cddd03a46e9b4d7daf85e6394126d2cd",
      "d90bbcac835c4c8e8aa819bc85c95f60",
      "203bfa2bf1a347a69f797de12faf8c52",
      "9b64d399019b43a8aa954df6667e4e0f",
      "3364d555d6f740b98dba6f27632a0b3b",
      "3f374378fd86437a837cf2891d11e523",
      "464e8210e9ad44588bf3aebfeb7f3e9f",
      "77c7a319874d40d383e05b94c2b2d2a3",
      "897f2fa4c4c540c2affd52a3baf00977",
      "67c560a88526489685316654be12dc57",
      "1d02921795a8479d9f3de8a747c64e50",
      "c5ec1e9ac10d4a87b07ecf8a206921d2",
      "dbdc707d13514e0ba9fa5c16b3aaa847",
      "bea26dd397a7485abf7790af6394955c",
      "7b2e70f29f414b32b4d56e1fe7e11199",
      "e01dfcc5fb234eaeac01da08ce2f0889",
      "b32de5619a6a4c6a802590f3c5f36c6d",
      "4861c9950a3840c08b1b2f467f9ba66d",
      "e87e1c106cf6436e80749c3614937221",
      "6bc8004b7e8442c48fe9e4403a0caf82",
      "9d19cb72f87d4b57bb42f3d41f2acb1e",
      "c7eee0a8d5814be2a4292446f1c8375a",
      "a6226930f16443b099364692102cbe73",
      "9e186da5b1b24aa48b85ed1fb9657743",
      "2660e6757b234300bbd05ee37b839de5",
      "32bdaf40b92a4f0d96917b9d3c09427d",
      "353e00019d3b4be0aca7b2718ab2b323",
      "b4b3c66c746240509f9a98b85930544d",
      "f2e70de8d421416fa842208a8ca7784c",
      "d87e5b0e84ec422cbdf2a556298bbf60",
      "7dc4705f62bc488bada12df568427143",
      "4518e9806151425c9589e9e9779a8c98",
      "03f62192de544373a08f3b9591aea17d",
      "109e720c03d34400bdefd8450fcf2f63",
      "a54c36983ca149f78e968194646c20f1",
      "14629a2311bc4668be08b455d56a18d0",
      "50968a9b8a83458a9b44e6eb56c5d4ad",
      "bb3443e9a6b146feaf7eb1d30c7b2366",
      "0d2ef5ee873946339fd0ad7ebbd7e83c",
      "da75469853cb40f5a5bf469b57b7c3ed",
      "0dcfb07dbfdc47a3a4970671d3f8ae21",
      "e1ae03fe855a4fd2b8ea2f0e2e3e18e6",
      "f422342bf6b8419fa0ccdc86050bc4b1",
      "019ce9e003ce49a4bce7827362108c5c",
      "a6b8465e585d457f9ddafb003af5fd8a",
      "860d7bd5e22544a89a0959f412d0d5d5",
      "0e98753b330f41fc999a79df044b3164",
      "477be0ad7c514c3e8f0e9706f6f5085e",
      "a60c8cf3ffe443e19a4392ca6fe3d85f",
      "e290c9b5dde444b89ed5f6218f0507be",
      "8b701a96be024f3a8c46fdf5f51ed944",
      "f8e95a718a4740d6ac0e191adfca0a70",
      "0bd98aabeffc442083d969aade154022",
      "1bde150136844686b42a1737177bde89",
      "352111d879044a6fa13e1d3548773da9",
      "660374c0414a4392b9d2b1d1b8a475bf"
     ]
    },
    "id": "lCUn4C-2zAOi",
    "outputId": "bc4fc7bd-7ab3-4374-d3a7-18da050bbee8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8953658fc654d139a6d1c277e178140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88c91b162a4470dbeb37abb1e8e7924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90bbcac835c4c8e8aa819bc85c95f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdc707d13514e0ba9fa5c16b3aaa847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e186da5b1b24aa48b85ed1fb9657743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54c36983ca149f78e968194646c20f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860d7bd5e22544a89a0959f412d0d5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Load the tokenizer and pretrained model\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9oYfTuRzV1z"
   },
   "source": [
    "## Preprocess the input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VKlRSaUUzPo7",
    "outputId": "6fd72915-d9f8-4ac4-ab94-f30105b84a03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   46,  1219,    11,   290,   673,   338,  7067,   257, 18761,  1014,\n",
       "           284, 11225]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "prompt = \"Ooh, and she's buying a stairway to Heaven\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v04GnmBJzho3"
   },
   "source": [
    "## Perform Inference  \n",
    "Generate text using the model\n",
    "\n",
    "```inputs:``` Input token IDs from the tokenizer\n",
    "\n",
    "```attention_mask:``` Mask indicating which tokens to attend to\n",
    "\n",
    "```pad_token_id:```Padding token ID set to the end-of-sequence token ID\n",
    "\n",
    "```max_length:``` Maximum length of the generated sequences\n",
    "\n",
    "```num_return_sequence:``` Number of sequences to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KO232cVkztHp",
    "outputId": "850424e1-04cd-4374-88c9-ccb8efcc5d9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   46,  1219,    11,   290,   673,   338,  7067,   257, 18761,  1014,\n",
       "           284, 11225,    13,   198,   198,     1,    40,  1101,  1016,   284,\n",
       "           467,   284, 11225,   553,   673,  1139,    13,   366,    40,  1101,\n",
       "          1016,   284,   467,   284, 11225,   526,   198,   198,     1,    40,\n",
       "          1101,  1016,   284,   467,   284, 11225,   553,   673,  1139,    13]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate text\n",
    "output_ids = model.generate(\n",
    "    inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eJRa8K3azxI5",
    "outputId": "c7bb4ed2-e1d3-47aa-9c2a-7e4b97ae4206"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ooh, and she's buying a stairway to Heaven.\n",
      "\n",
      "\"I'm going to go to Heaven,\" she says. \"I'm going to go to Heaven.\"\n",
      "\n",
      "\"I'm going to go to Heaven,\" she says.\n"
     ]
    }
   ],
   "source": [
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhfltOGuze4U"
   },
   "source": [
    "# Hugging Face `pipeline()` function\n",
    "\n",
    "The `pipeline()` function from the Hugging Face `transformers` library is a high-level API designed to simplify the usage of pretrained models for various natural language processing (NLP) tasks. It abstracts the complexities of model loading, tokenization, inference, and post-processing, allowing users to perform complex NLP tasks with just a few lines of code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoqYUdda2Gix"
   },
   "source": [
    "## Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygNcKcqC1ryq"
   },
   "source": [
    "### Definition\n",
    "\n",
    "```python\n",
    "transformers.pipeline(\n",
    "    task: str,\n",
    "    model: Optional = None,\n",
    "    config: Optional = None,\n",
    "    tokenizer: Optional = None,\n",
    "    feature_extractor: Optional = None,\n",
    "    framework: Optional = None,\n",
    "    revision: str = 'main',\n",
    "    use_fast: bool = True,\n",
    "    model_kwargs: Dict[str, Any] = None,\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **task**: `str`\n",
    "  - The task to perform, such as \"text-classification\", \"text-generation\", \"question-answering\", etc.\n",
    "  - Example: `\"text-classification\"`\n",
    "\n",
    "- **model**: `Optional`\n",
    "  - The model to use. This can be a string (model identifier from Hugging Face model hub), a path to a directory containing model files, or a pre-loaded model instance.\n",
    "  - Example: `\"distilbert-base-uncased-finetuned-sst-2-english\"`\n",
    "\n",
    "- **config**: `Optional`\n",
    "  - The configuration to use. This can be a string, a path to a directory, or a pre-loaded config object.\n",
    "  - Example: `{\"output_attentions\": True}`\n",
    "\n",
    "- **tokenizer**: `Optional`\n",
    "  - The tokenizer to use. This can be a string, a path to a directory, or a pre-loaded tokenizer instance.\n",
    "  - Example: `\"bert-base-uncased\"`\n",
    "\n",
    "- **feature_extractor**: `Optional`\n",
    "  - The feature extractor to use for tasks that require it (e.g., image processing).\n",
    "  - Example: `\"facebook/detectron2\"`\n",
    "\n",
    "- **framework**: `Optional`\n",
    "  - The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. If not specified, it will be inferred.\n",
    "  - Example: `\"pt\"`\n",
    "\n",
    "- **revision**: `str`, default `'main'`\n",
    "  - The specific model version to use (branch, tag, or commit hash).\n",
    "  - Example: `\"v1.0\"`\n",
    "\n",
    "- **use_fast**: `bool`, default `True`\n",
    "  - Whether to use the fast version of the tokenizer if available.\n",
    "  - Example: `True`\n",
    "\n",
    "- **model_kwargs**: `Dict[str, Any]`, default `None`\n",
    "  - Additional keyword arguments passed to the model during initialization.\n",
    "  - Example: `{\"output_hidden_states\": True}`\n",
    "\n",
    "- **kwargs**: `Any`\n",
    "  - Additional keyword arguments passed to the pipeline components.\n",
    "\n",
    "### Task types\n",
    "\n",
    "The `pipeline()` function supports a wide range of NLP tasks. Here are some of the common tasks:\n",
    "\n",
    "1. **Text Classification**: `text-classification`\n",
    "   - **Purpose**: Classify text into predefined categories.\n",
    "   - **Use Cases**: Sentiment analysis, spam detection, topic classification.\n",
    "\n",
    "2. **Text Generation**: `text-generation`\n",
    "   - **Purpose**: Generate coherent text based on a given prompt.\n",
    "   - **Use Cases**: Creative writing, dialogue generation, story completion.\n",
    "\n",
    "3. **Question Answering**: `question-answering`\n",
    "   - **Purpose**: Answer questions based on a given context.\n",
    "   - **Use Cases**: Building Q&A systems, information retrieval from documents.\n",
    "\n",
    "4. **Named Entity Recognition (NER)**: `ner` (or `token-classification`)\n",
    "   - **Purpose**: Identify and classify named entities (like people, organizations, locations) in text.\n",
    "   - **Use Cases**: Extracting structured information from unstructured text.\n",
    "\n",
    "5. **Summarization**: `summarization`\n",
    "   - **Purpose**: Summarize long pieces of text into shorter, coherent summaries.\n",
    "   - **Use Cases**: Document summarization, news summarization.\n",
    "\n",
    "6. **Translation**: `translation_xx_to_yy` (e.g., `translation_en_to_fr`)\n",
    "   - **Purpose**: Translate text from one language to another.\n",
    "   - **Use Cases**: Language translation, multilingual applications.\n",
    "\n",
    "7. **Fill-Mask**: `fill-mask`\n",
    "   - **Purpose**: Predict masked words in a sentence (useful for masked language modeling).\n",
    "   - **Use Cases**: Language modeling tasks, understanding model predictions.\n",
    "\n",
    "8. **Zero-Shot Classification**: `zero-shot-classification`\n",
    "   - **Purpose**: Classify text into categories without needing training data for those categories.\n",
    "   - **Use Cases**: Flexible and adaptable classification tasks.\n",
    "\n",
    "9. **Feature Extraction**: `feature-extraction`\n",
    "   - **Purpose**: Extract hidden state features from text.\n",
    "   - **Use Cases**: Downstream tasks requiring text representations, such as clustering, similarity, or further custom model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gmjRHPj1gsy"
   },
   "source": [
    "## Example 1: Text Classification using `pipeline()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loR1dzz62SjC",
    "outputId": "b13d07fd-606b-4786-827f-80856826597e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# @title Load a general text classification model\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_m_RF3u2rAt",
    "outputId": "f5401278-f110-4314-9198-e2f5fd9d2b44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.999832034111023}]\n"
     ]
    }
   ],
   "source": [
    "# @title Classify a sample text\n",
    "result = classifier(\"Their innovative blend of thunderous riffs, soaring vocals, and mystical lyricism established Led Zeppelin as pioneers, forever altering the trajectory of popular music.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7eTs9Ptl3Ggh",
    "outputId": "c9bf6569-0649-4010-97b8-00298fe159b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9997269511222839}]\n"
     ]
    }
   ],
   "source": [
    "# @title Classify another sample text\n",
    "result = classifier(\"Nickelback's music is a masterclass in generic, focus-grouped rock, delivering formulaic anthems devoid of genuine artistry.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GX7WZJpR3uYd"
   },
   "source": [
    "## Example 2: Language detection using `pipeline()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245,
     "referenced_widgets": [
      "f9b5cc350ceb4fed913318393df8cfcf",
      "078845ba9d8f42d9b21e3a5a6ff20819",
      "25b00d75f8c34b5895ea49c796de2fdc",
      "26a1d14f8f6742539c15c67cf3311dd6",
      "2b6e87d45d7d40e6b1c065dae5455d2c",
      "5d894de96d49497ba36fea1c04e90089",
      "cd724efdef26440ea9be63eddf5f8eba",
      "c973068ad55f44c28ddbbd98350d55a5",
      "72bf68bfdade4280a6595cda89dd0824",
      "c11e40a0d89c486fb5eb4b995a844ebb",
      "6ff139770b63446c82ab9c878a2ec183",
      "049325e9612e499a9fe033926cfcda2b",
      "38f5d35acd974e47bc22b811b911aa8d",
      "7c998fd6bf88483f856ea9417feab42b",
      "fa90708145144f50820784db3f5378a6",
      "6a753386d28549a7b2fdb8ac1fdcd8e8",
      "71b868e2a65b44118cae0b14d112ef13",
      "78a7f8eb740c40a2957568eb513c5b14",
      "ba64590915d04422be83c9e5b827c8c8",
      "0a6f6caa7ef343b18fe229a021b2b77f",
      "5fe57719d28e43e0a1ec7d00da732501",
      "3b18e690c0f44ef69c953e3ffbb8833d",
      "e65d784a5d4e49f7b3edbcee60c862f8",
      "4f25ba4f155048a3931b8aa4b4bd6b0e",
      "63c3f3a01efc44378e02b581130e5584",
      "a6bcc03306a547a78fdf2e4c5a74e592",
      "213093c6be1c45f3a802bf7b48ea2a80",
      "ed349effc13d4de6850a4af18e1ef47f",
      "c7703a894d98436aa839ce71acd69c51",
      "9e1eacee2a324507a2a374c0bc9a65ee",
      "f1062f3f376349c39e918f51b0a6b32e",
      "08900a99458f4ce1a6f9c808d9fdb80d",
      "7f37d564323342b5a23201b4051ed03b",
      "77c9284fd5504ed2b8342a561592d864",
      "874d14db41e64f37a64fb382b5729acd",
      "b82b9e7050a5486196238164b92fdc8c",
      "2cd909a6f8b44268b9b7e19b868149e7",
      "35b979765b4e413493651bf877f931dc",
      "6ffa4b01622f40968f9454405a904be2",
      "dbaacb9c70d449dda208bb083a60783a",
      "c3633ac16ea14ed3b218821b89d2f61c",
      "c5695b74aee847838b7bc4be690b9230",
      "ec5c1c9edf6f451ba144b8bd45985050",
      "00b57f9f63c44b1c87a7a9c7e44cb7ee",
      "35b7754e8b344de68223ea2c368c9eab",
      "7789ece37fb84d71bba71838b81c0adc",
      "4d1659cde69545c2a66dd4f36b4a87c9",
      "dc899656e8064641af355f956da2000c",
      "fb94a2496f6346f790a7de8d59a3ebdc",
      "9e9e8f7e0c354f2cb46ffdcf9d2e0cc8",
      "f3493229549e40f4a00fe68a859615e9",
      "4838bcdecc7f484ba23a1cf67c2863c7",
      "d55815b0f5e548e9b3e7d7474a930998",
      "b3657ecb5f554f7db392faf39d0c66d5",
      "4695f3828f6843c48dce535efc6962e3",
      "aa6d17069e48498bbf3bac47d39bb54b",
      "8b0630274d1c4750b6b76b9ae307a1ef",
      "0896d31e7213417794c3c49cf2e808d0",
      "a25d3be7c4cd4aca96555f17ebe54766",
      "8d6a4e5a67aa457db4ae468c995e1f8e",
      "8f859ebb134848b88b8df7bb21221c98",
      "a0df52b60a3e40fa8eb8da41417e644f",
      "923666d1b26f41269f8dac4a1a4d8a5e",
      "37e76411260541559866476f8479df6d",
      "070c600b906e48b784460c0b076f8cea",
      "6db20ec54415472dac955cf5fd33ed2b"
     ]
    },
    "id": "T4JT8kKx4B9b",
    "outputId": "2e44102e-f54c-4133-c386-144964b92970"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b5cc350ceb4fed913318393df8cfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049325e9612e499a9fe033926cfcda2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65d784a5d4e49f7b3edbcee60c862f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c9284fd5504ed2b8342a561592d864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b7754e8b344de68223ea2c368c9eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6d17069e48498bbf3bac47d39bb54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'fr', 'score': 0.9934879541397095}]\n"
     ]
    }
   ],
   "source": [
    "# Load the language detection model\n",
    "classifier = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\")\n",
    "\n",
    "# Classify the sample text\n",
    "result = classifier(\"Bonjour, comment ça va?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pp15Mfhf3xFs"
   },
   "source": [
    "## Example 3: Text Generation using `pipeline()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29Wl70f_4qKr",
    "outputId": "942569a9-7212-470b-de91-f2f73ab142be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text generation pipeline with GPT-2\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aLTLV_TZ4xig",
    "outputId": "1e973841-796a-4ae4-fd8e-0451c9d2afc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We came from the land of ice and snow. We were so much stronger than anyone we never had a chance to fight.\n",
      "\n",
      "\"To be honest, we were always fighting for each other.\n"
     ]
    }
   ],
   "source": [
    "# Generate text based on a given prompt\n",
    "prompt = \"We came from the land of ice and snow\"\n",
    "result = generator(prompt, max_length=40,  max_new_tokens=None, num_return_sequences=1, truncation=True, pad_token_id=50256)\n",
    "\n",
    "# Print the generated text\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ACCn5nl4hOA"
   },
   "source": [
    "## Example 4: Text generation using T5 with `pipeline()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227,
     "referenced_widgets": [
      "1fb510ae024d4700a536aeb36f1537fc",
      "0087cd78da1b48db84a226234f9a28ba",
      "1eba75213b634b4d9fa90b2b4dbe8216",
      "2be44e8d7eca41e1a0db65a5ac072cce",
      "70294d43325b4e8ab8c38e401b3b985c",
      "00272804656b4f6eacbce061f44b2f5b",
      "dd4d833187db4f63a6f2082af6c4eef4",
      "8ee48c100fbc429694544f800bd70c29",
      "1450288ba2294d54a3c7598bc51b203d",
      "b21a3f2ec8544295bdfe983f001f50e3",
      "73107f7684194184acdc144cfb0c12d2",
      "4d15334e6747462d9ed9361296278416",
      "3d40aa1bead549519a744caf2697f1b0",
      "8fd5179040b8441cb21a1d27775b70d9",
      "15c6e64bf55545fab3d4241266c3c12a",
      "eb6343bd746848dfb09b55353a8a688b",
      "2d19b78d74ca4547a994b41414d99de4",
      "7f2df1f40e3a4222827b86209c2adaa6",
      "cdbc97cdb428410c964b444729d0db9b",
      "b0be97f1cfbe4df4bca426e770d0905c",
      "3fad3a19acfb4a5b9ddf1e9397291499",
      "73078829da9f481aa5b5710c4c53b00c",
      "846812c0714c46de8714d1108fdd9475",
      "ae1563d157884b498c2cb20a9706adb0",
      "884b2658c57a41fabc62b3d49b5c62ab",
      "f665d49a7a9841f9889b8c9e607f6401",
      "a735982dc2754ae39a1653bd9001a1f8",
      "c96a856be12c4286909f21e23cfe5c49",
      "a8a9802a44e44067b09446b5af3008f2",
      "e1906e9d4173415db07c700466e4c2f4",
      "f8ddc5459d824a46a7b4499796056ed5",
      "62e6c9729d6e416ca9f9b5ca6969f237",
      "b3945d3ff32b4e8fab96eb36146d80cd",
      "f418e9afe4334a48bab99172b9cfeafc",
      "2c252bae2c72468f9c2b5bd559523d80",
      "8c0a4a41a9314805971c4f6f23c5150f",
      "d4ab106848b24d7bb2da84a75bfb7720",
      "094ebb57fb46483bb80f428cba3bba60",
      "585aa553aec149c28695b830631b7976",
      "d92f732fd5a74925b6e69a5b5e17b9d8",
      "6ed10f8476af46f29b3403a091c5ee12",
      "e3be808419b6432198ca13cd5d34ee2d",
      "8ce3eeac4630412f87646adb5bef5722",
      "635b7a51995943af8a78f05622557736",
      "0e1305987fa341969904b0e8ac73bb01",
      "8abafa6f9ca6497fb721d2f2b591b6fb",
      "87aba268b5ec43eba680572869f7d0bd",
      "b85ef24ca8614b99beb8a22e730e8e7c",
      "a0dc35be3d8c478e9fd579a3c4aeb73f",
      "b92e325311d14f67b79425fc41f645b3",
      "00de424e734d49e49f371346c9e84843",
      "8e0eb69d9f364b9c8ec38f93898d935d",
      "00f64ecb3af747689965df93f02ffc88",
      "a4f78c6248d141318fef7ee27d347b3b",
      "8a097e7facd84967a8585d9ec3055ddd",
      "6be8a595e481470c9b025c5b8414450c",
      "536c565a4c834d339292ee78164d04b4",
      "726ec157171c4936bd535ad889528e9f",
      "90e9469a95c541c79bfe172b03434649",
      "58fc7a2e3ba440aab7a716a0ecb3f5f4",
      "5ec8b16e57d24558ab33b0a3f48fb749",
      "f84070f0089c4db5a2d913e138bb674a",
      "47bfd3f2109b47e1af81ba108ab89553",
      "8e6b4b457bde4cff878cf6abb41ebce6",
      "d7d36eb9caa34881952619c54ddb2281",
      "cfdb63e3bab3411fbbdff246113c23e9"
     ]
    },
    "id": "9IbjVmEL6r6Q",
    "outputId": "a09ab8b1-7234-4a18-a2f0-bbf985783da5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb510ae024d4700a536aeb36f1537fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d15334e6747462d9ed9361296278416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846812c0714c46de8714d1108fdd9475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f418e9afe4334a48bab99172b9cfeafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1305987fa341969904b0e8ac73bb01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be8a595e481470c9b025c5b8414450c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text generation pipeline with T5\n",
    "generator = pipeline(\"text2text-generation\", model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9KnnKz3j68uf",
    "outputId": "0f63816d-fa3a-419a-a62b-95a429d12bc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment êtes-vous?\n"
     ]
    }
   ],
   "source": [
    "# Generate text based on a given prompt\n",
    "prompt = \"translate English to French: How are you?\"\n",
    "result = generator(prompt, max_length=50, max_new_tokens=None, num_return_sequences=1)\n",
    "\n",
    "# Print the generated text\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "iufLRqPOy4jl",
    "8zNHlHTDuTWx",
    "Tk4hSh-2yzre",
    "uoqYUdda2Gix",
    "8gmjRHPj1gsy",
    "GX7WZJpR3uYd"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
