{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7c03451c",
      "metadata": {
        "id": "7c03451c"
      },
      "source": [
        "# CoverLetter AI Generator\n",
        "\n",
        "This notebook integrates LangGraph's `StateGraph` with the Groq API to generate personalized cover letters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13959ebe",
      "metadata": {
        "id": "13959ebe"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install required packages\n",
        "!pip install -qq gradio > /dev/null 2>&1\n",
        "!pip install -qqU langgraph > /dev/null 2>&1\n",
        "!pip install -qq pymupdf python-docx pandas openpyxl typing-extensions> /dev/null 2>&1\n",
        "!pip install -qq groq > /dev/null 2>&1\n",
        "\n",
        "# Suppress warnings & logs\n",
        "import warnings, logging\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"âœ… Environment ready (quiet mode)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDmW9WRFtgMG",
        "outputId": "12cc479f-bc8d-4840-d005-9e5f81ec12a5"
      },
      "id": "wDmW9WRFtgMG",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Environment ready (quiet mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4c973a1c",
      "metadata": {
        "id": "4c973a1c"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "import os\n",
        "os.environ[\"USER_AGENT\"] = \"CoverLetter_AI/1.0 (contact: pricotu@outlook.com)\"\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, TypedDict, Any\n",
        "from typing_extensions import TypedDict, NotRequired\n",
        "from pathlib import Path\n",
        "import re\n",
        "import tempfile\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# LangGraph imports\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# Groq import\n",
        "from groq import Groq\n",
        "\n",
        "# Gradio\n",
        "import gradio as gr\n",
        "\n",
        "# File libs\n",
        "import fitz\n",
        "import docx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Configuration\n",
        "@dataclass\n",
        "class Config:\n",
        "    GROQ_API_KEY: str = os.getenv('GROQ_API_KEY','API_KEY_HERE')\n",
        "    MODEL_NAME: str = os.getenv('GROQ_MODEL','llama-3.3-70b-versatile')\n",
        "    MAX_TOKENS: int = 2500\n",
        "    TEMPERATURE: float = 0.7\n",
        "\n",
        "config = Config()\n",
        "print('Config loaded. Model:', config.MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-gt6QWtuv1h",
        "outputId": "810ba493-4f29-42d7-b03f-d3cba5b37821"
      },
      "id": "O-gt6QWtuv1h",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config loaded. Model: llama-3.3-70b-versatile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BACK END"
      ],
      "metadata": {
        "id": "aCuOHy9evBNT"
      },
      "id": "aCuOHy9evBNT"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c188c812",
      "metadata": {
        "id": "c188c812"
      },
      "outputs": [],
      "source": [
        "# @title Groq client wrapper (simple string output)\n",
        "class GroqClient:\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        self.api_key = api_key or config.GROQ_API_KEY\n",
        "        if not self.api_key:\n",
        "            print('[GroqClient] Warning: GROQ_API_KEY not set; using placeholder responses.')\n",
        "            self.client = None\n",
        "        else:\n",
        "            if Groq is None:\n",
        "                raise ImportError('groq package not installed or importable.')\n",
        "            self.client = Groq(api_key=self.api_key)\n",
        "    def generate(self, prompt: str, system_prompt: Optional[str] = None, max_tokens: Optional[int] = None) -> str:\n",
        "        # Simple: return only text\n",
        "        try:\n",
        "            messages = []\n",
        "            if system_prompt:\n",
        "                messages.append({'role': 'system', 'content': system_prompt})\n",
        "            messages.append({'role': 'user', 'content': prompt})\n",
        "            if self.client is None:\n",
        "                # placeholder response for offline testing\n",
        "                return '[groq placeholder] ' + prompt[:400]\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=config.MODEL_NAME,\n",
        "                messages=messages,\n",
        "                max_tokens=max_tokens or config.MAX_TOKENS,\n",
        "                temperature=config.TEMPERATURE\n",
        "            )\n",
        "            # The Groq response object shape may vary; adapt if necessary.\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            return f'Error generating response: {str(e)}'\n",
        "\n",
        "# instantiate\n",
        "llm = GroqClient()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "42499b78",
      "metadata": {
        "id": "42499b78"
      },
      "outputs": [],
      "source": [
        "# @title File processing utilities\n",
        "class FileProcessor:\n",
        "    @staticmethod\n",
        "    def extract_text_from_pdf(path: str) -> str:\n",
        "        doc = fitz.open(path)\n",
        "        text = '\\n'.join([page.get_text() for page in doc])\n",
        "        doc.close()\n",
        "        return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_text_from_docx(path: str) -> str:\n",
        "        doc = docx.Document(path)\n",
        "        text = '\\n'.join([p.text for p in doc.paragraphs])\n",
        "        return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_text(path: str):\n",
        "        if not path:\n",
        "            return '', ''\n",
        "        p = Path(path)\n",
        "        ext = p.suffix.lower()\n",
        "        if ext == '.pdf':\n",
        "            return FileProcessor.extract_text_from_pdf(path), p.name\n",
        "        if ext == '.docx':\n",
        "            return FileProcessor.extract_text_from_docx(path), p.name\n",
        "        if ext == '.txt':\n",
        "            return open(path, 'r', encoding='utf-8').read(), p.name\n",
        "        return '', p.name\n",
        "\n",
        "    @staticmethod\n",
        "    def load_excel_data(path: str) -> pd.DataFrame:\n",
        "      if not path or not os.path.exists(path):\n",
        "          return pd.DataFrame()\n",
        "\n",
        "      try:\n",
        "          all_sheets = pd.read_excel(path, sheet_name=None)\n",
        "          dfs = []\n",
        "\n",
        "          for sheet, df in all_sheets.items():\n",
        "              df = df.copy()\n",
        "              df['__sheet__'] = sheet\n",
        "              dfs.append(df)\n",
        "\n",
        "          if dfs:\n",
        "              return pd.concat(dfs, ignore_index=True)\n",
        "          return pd.DataFrame()\n",
        "      except Exception as e:\n",
        "          print(f\"Error loading Excel file: {e}\")\n",
        "          return pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Function to clean Skill Lists\n",
        "def clean_and_parse_skills(text: str) -> List[str]:\n",
        "    \"\"\"Clean and parse skills from response\"\"\"\n",
        "    cleanup_phrases = [\n",
        "        \"Here are the extracted skills as a comma-separated list:\",\n",
        "        \"Here are the professional skills extracted\",\n",
        "        \"Here are the skills:\",\n",
        "        \"Skills:\",\n",
        "        \"The skills are:\"\n",
        "    ]\n",
        "\n",
        "    cleaned_text = text.strip()\n",
        "    for phrase in cleanup_phrases:\n",
        "        cleaned_text = cleaned_text.replace(phrase, \"\").strip()\n",
        "\n",
        "    # Split by comma and clean each skill\n",
        "    if ',' in cleaned_text:\n",
        "        skills = [s.strip() for s in cleaned_text.split(',')]\n",
        "    else:\n",
        "        skills = [s.strip() for s in cleaned_text.split('\\n')]\n",
        "\n",
        "    # Filter and clean skills\n",
        "    cleaned_skills = []\n",
        "    for skill in skills:\n",
        "        skill = skill.strip().strip('â€¢').strip('-').strip()\n",
        "        if (skill and len(skill) > 1 and len(skill) < 60 and\n",
        "            not skill.lower().startswith(('here', 'the', 'extract'))):\n",
        "            cleaned_skills.append(skill)\n",
        "\n",
        "    return cleaned_skills[:30]"
      ],
      "metadata": {
        "id": "jIA8kOMVaMmO"
      },
      "id": "jIA8kOMVaMmO",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "938a7dd1",
      "metadata": {
        "id": "938a7dd1"
      },
      "outputs": [],
      "source": [
        "# @title Define the shared State schema for StateGraph\n",
        "class State(TypedDict, total=False):\n",
        "    # Original fields\n",
        "    cv_text: NotRequired[str]\n",
        "    candidate_name: NotRequired[str]\n",
        "    job_text: NotRequired[str]\n",
        "    job_description: NotRequired[str]\n",
        "    company: NotRequired[str]\n",
        "    title: NotRequired[str]\n",
        "    applications_df: NotRequired[Any]  # pandas DataFrame\n",
        "    skills_summary: NotRequired[str]\n",
        "    cv_skills: NotRequired[list]\n",
        "    job_skills: NotRequired[list]\n",
        "    matched_skills: NotRequired[list]\n",
        "    unmatched_skills: NotRequired[list]\n",
        "    matching_score: NotRequired[int]\n",
        "    history_note: NotRequired[str]\n",
        "    cover_letter: NotRequired[str]\n",
        "    cv_relevant_sections: NotRequired[str]  # Output from cv_section_extractor_node\n",
        "    overall_skill_match: NotRequired[int]   # Output from skill_matching_node\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "34dad9c7",
      "metadata": {
        "id": "34dad9c7"
      },
      "outputs": [],
      "source": [
        "# @title Agent functions\n",
        "\n",
        "def extract_candidate_info(cv_text: str) -> str:\n",
        "    \"\"\"Extract candidate name and title from CV text\"\"\"\n",
        "    if not cv_text:\n",
        "        return \"Name not found\"\n",
        "\n",
        "    system = \"Extract the candidate's full name and professional title from this CV. Return in format: 'Name - Title'\"\n",
        "    prompt = f\"Extract name and title from this CV:\\n{cv_text[:1000]}\"\n",
        "\n",
        "    try:\n",
        "        response = llm.generate(prompt, system_prompt=system, max_tokens=100)\n",
        "        return response.strip()\n",
        "    except:\n",
        "        # Fallback: look for name patterns in first few lines\n",
        "        lines = cv_text.split('\\n')[:5]\n",
        "        for line in lines:\n",
        "            if len(line.strip()) > 5 and len(line.strip()) < 50:\n",
        "                return line.strip()\n",
        "        return \"Name not found\"\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "def job_extraction_node(state: State) -> State:\n",
        "    \"\"\"Extract company, title, and description from job text\"\"\"\n",
        "    job_text = state.get('job_text', '')\n",
        "\n",
        "    system = '''Extract company name, job title, and clean description from the job posting.\n",
        "    Return as JSON with keys: company, title, description. Return valid JSON only.'''\n",
        "\n",
        "    prompt = f\"Job posting text:\\n{job_text[:8000]}\"\n",
        "\n",
        "    resp = llm.generate(prompt, system_prompt=system, max_tokens=800)\n",
        "\n",
        "    # Try to parse JSON response\n",
        "    try:\n",
        "        parsed = json.loads(resp)\n",
        "        return {\n",
        "            'company': parsed.get('company', 'Unknown Company'),\n",
        "            'title': parsed.get('title', 'Unknown Position'),\n",
        "            'job_description': parsed.get('description', job_text[:2000])\n",
        "        }\n",
        "    except Exception:\n",
        "        # Fallback to simple heuristics\n",
        "        lines = job_text.split('\\n')\n",
        "        company = lines[0] if lines else 'Unknown Company'\n",
        "        title = lines[1] if len(lines) > 1 else 'Unknown Position'\n",
        "        description = '\\n'.join(lines[2:]) if len(lines) > 2 else job_text[:2000]\n",
        "\n",
        "        return {\n",
        "            'company': company.strip(),\n",
        "            'title': title.strip(),\n",
        "            'job_description': description.strip()\n",
        "        }\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "def history_node(state: State) -> State:\n",
        "    \"\"\"Check application history for the company\"\"\"\n",
        "    df = state.get('applications_df', pd.DataFrame())\n",
        "    company = state.get('company', '')\n",
        "\n",
        "    if df is None or df.empty:\n",
        "        return {'history_note': 'No application history provided'}\n",
        "\n",
        "    # Find company-like columns\n",
        "    cols = [c for c in df.columns if isinstance(c, str)]\n",
        "    company_cols = [c for c in cols if any(word in c.lower() for word in ['company', 'business', 'employer', 'organization'])]\n",
        "\n",
        "    if not company_cols:\n",
        "        company_cols = [cols[0]] if cols else []\n",
        "\n",
        "    if not company_cols:\n",
        "        return {'history_note': 'No company column found in application history'}\n",
        "\n",
        "    company_col = company_cols[0]\n",
        "\n",
        "    # Search for company matches\n",
        "    try:\n",
        "        matches = df[df[company_col].astype(str).str.contains(company, case=False, na=False)]\n",
        "\n",
        "        if matches.empty:\n",
        "            return {'history_note': f'No previous applications found for {company}'}\n",
        "        else:\n",
        "            recent = matches.head(3)\n",
        "            history_text = f'Found {len(matches)} previous application(s) to {company}:\\n'\n",
        "            for _, row in recent.iterrows():\n",
        "                date = row.get('Date', 'Unknown date')\n",
        "                response = row.get('Response', 'No response recorded')\n",
        "                history_text += f\"- {date}: {response}\\n\"\n",
        "\n",
        "            return {'history_note': history_text}\n",
        "    except Exception as e:\n",
        "        return {'history_note': f'Error checking application history: {str(e)}'}\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "def writer_node(state: State) -> State:\n",
        "    \"\"\"Generate the cover letter using AIDA framework\"\"\"\n",
        "    company = state.get('company', 'Unknown Company')\n",
        "    title = state.get('title', 'Unknown Position')\n",
        "    job_desc = state.get('job_description', '')[:1600]\n",
        "    cv_text = state.get('cv_text', '')[:1600]\n",
        "    matched_skills_raw = state.get('matched_skills', [])\n",
        "    history = state.get('history_note', '')\n",
        "\n",
        "    # FIX: Extract skill names from dictionaries\n",
        "    matched_skills = []\n",
        "    for skill_item in matched_skills_raw[:10]:  # Top 10 skills\n",
        "        if isinstance(skill_item, dict):\n",
        "            matched_skills.append(skill_item.get('skill', str(skill_item)))\n",
        "        else:\n",
        "            matched_skills.append(str(skill_item))\n",
        "\n",
        "    system = '''You are an expert career consultant. Write a professional cover letter using the AIDA framework (Attention, Interest, Desire, Action).\n",
        "    Keep it concise (3-4 paragraphs, 250-350 words). Make it personalized and compelling.'''\n",
        "\n",
        "    prompt = f\"\"\"Write a cover letter for:\n",
        "Position: {title}\n",
        "Company: {company}\n",
        "\n",
        "Job Description: {job_desc}\n",
        "\n",
        "Candidate Background: {cv_text}\n",
        "\n",
        "Key Matching Skills: {', '.join(matched_skills)}\n",
        "\n",
        "Application History: {history}\n",
        "\n",
        "Create a personalized, professional cover letter that highlights relevant experience and enthusiasm for the role.\"\"\"\n",
        "\n",
        "    response = llm.generate(prompt, system_prompt=system, max_tokens=1000)\n",
        "\n",
        "    return {'cover_letter': response}\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "def cv_section_extractor_node(state: State) -> State:\n",
        "    \"\"\"Extract relevant sections from CV for skills analysis\"\"\"\n",
        "    cv_text = state.get('cv_text', '')\n",
        "\n",
        "    system = \"\"\"Extract the most relevant sections from this CV for skills analysis:\n",
        "    1. Skills/Technical Skills section\n",
        "    2. Work Experience section\n",
        "    3. Education/Certifications section\n",
        "    4. Soft Skills section\n",
        "\n",
        "    Return only the text content from these sections, clearly labeled.\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"Extract and return the following sections from this CV:\n",
        "\n",
        "{cv_text}\n",
        "\n",
        "Format as:\n",
        "SKILLS SECTION:\n",
        "[content]\n",
        "\n",
        "WORK EXPERIENCE:\n",
        "[content]\n",
        "\n",
        "CERTIFICATIONS:\n",
        "[content]\n",
        "\n",
        "SOFT SKILLS:\n",
        "[content]\"\"\"\n",
        "\n",
        "    response = llm.generate(prompt, system_prompt=system, max_tokens=1500)\n",
        "\n",
        "    return {'cv_relevant_sections': response}\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "def job_skills_extractor_node(state: State) -> State:\n",
        "    \"\"\"Extract skills from job description\"\"\"\n",
        "    job_desc = state.get('job_description', '')[:6000]\n",
        "\n",
        "    system = \"\"\"Extract 25-30 required professional skills from this job description. Focus on:\n",
        "    1. Technical requirements (programming languages, tools, frameworks)\n",
        "    2. Methodologies (Agile, DevOps, etc.)\n",
        "    3. Domain expertise\n",
        "    4. Soft skills requirements\n",
        "\n",
        "    Return ONLY a clean comma-separated list. No introductory text.\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"Extract required skills from this job description:\n",
        "\n",
        "{job_desc}\n",
        "\n",
        "Return exactly as: skill1, skill2, skill3, etc.\"\"\"\n",
        "\n",
        "    response = llm.generate(prompt, system_prompt=system, max_tokens=400)\n",
        "\n",
        "    # Clean the response\n",
        "    job_skills = clean_and_parse_skills(response)\n",
        "\n",
        "    return {'job_skills': job_skills}\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "def cv_skills_extractor_node(state: State) -> State:\n",
        "    \"\"\"Extract skills from relevant CV sections\"\"\"\n",
        "    cv_sections = state.get('cv_relevant_sections', '')\n",
        "\n",
        "    system = \"\"\"Extract 40 - 50 professional skills from these CV sections. Look for:\n",
        "    1. Explicitly listed skills\n",
        "    2. Technologies mentioned in work experience\n",
        "    3. Programming languages and tools\n",
        "    4. Certifications and training\n",
        "    5. Soft skills mentioned\n",
        "\n",
        "    Return ONLY a clean comma-separated list. No introductory text.\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"Extract skills from these relevant CV sections:\n",
        "\n",
        "{cv_sections}\n",
        "\n",
        "Return exactly as: skill1, skill2, skill3, etc.\"\"\"\n",
        "\n",
        "    response = llm.generate(prompt, system_prompt=system, max_tokens=800)\n",
        "\n",
        "    # Clean the response\n",
        "    cv_skills = clean_and_parse_skills(response)\n",
        "\n",
        "    return {'cv_skills': cv_skills}\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "def skill_matching_node(state: State) -> State:\n",
        "    \"\"\"Match job skills against CV sections with detailed analysis\"\"\"\n",
        "    job_skills = state.get('job_skills', [])\n",
        "    cv_sections = state.get('cv_relevant_sections', '')\n",
        "    cv_skills = state.get('cv_skills', [])\n",
        "\n",
        "    skills_str = \"\\n\".join([f\"- {s}\" for s in job_skills])\n",
        "\n",
        "    prompt = f\"\"\"You are an expert recruiter analyzing skill matches between job requirements and a candidate's CV.\n",
        "\n",
        "JOB REQUIRED SKILLS:\n",
        "{skills_str}\n",
        "\n",
        "CANDIDATE'S RELEVANT CV SECTIONS:\n",
        "{cv_sections}\n",
        "\n",
        "CANDIDATE'S EXTRACTED SKILLS:\n",
        "{', '.join(cv_skills)}\n",
        "\n",
        "For each job skill, determine:\n",
        "1. Does the candidate have this skill (based on CV evidence)?\n",
        "2. Similarity score (0-100) based on direct mentions, related experience, or transferable skills\n",
        "3. Brief reason explaining the match/mismatch\n",
        "\n",
        "Return valid JSON array:\n",
        "[\n",
        "  {{\"skill\": \"Python\", \"similarity\": 95, \"match\": true, \"reason\": \"Listed in skills section and used in multiple projects\"}},\n",
        "  {{\"skill\": \"Kubernetes\", \"similarity\": 30, \"match\": false, \"reason\": \"No direct mention, but has Docker experience\"}}\n",
        "]\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm.generate(prompt, max_tokens=1200)\n",
        "        # Clean response to ensure valid JSON\n",
        "        response = response.strip()\n",
        "        if not response.startswith('['):\n",
        "            # Extract JSON from response if there's extra text\n",
        "            import re\n",
        "            json_match = re.search(r'\\[.*\\]', response, re.DOTALL)\n",
        "            if json_match:\n",
        "                response = json_match.group(0)\n",
        "\n",
        "        results = json.loads(response)\n",
        "\n",
        "        matched = [r for r in results if r.get(\"match\", False)]\n",
        "        unmatched = [r for r in results if not r.get(\"match\", False)]\n",
        "\n",
        "        overall_match = round((len(matched) / len(job_skills)) * 100, 2) if job_skills else 0\n",
        "\n",
        "        return {\n",
        "            \"matched_skills\": matched,\n",
        "            \"unmatched_skills\": unmatched,\n",
        "            \"overall_skill_match\": overall_match\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in skill matching: {e}\")\n",
        "        return {\n",
        "            \"matched_skills\": [],\n",
        "            \"unmatched_skills\": job_skills,\n",
        "            \"overall_skill_match\": 0\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5f32e842",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f32e842",
        "outputId": "96d99984-6364-4cb2-ca02-875f84a07b5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph workflow compiled successfully\n"
          ]
        }
      ],
      "source": [
        "# @title Build StateGraph for the Agents\n",
        "builder = StateGraph(State)\n",
        "\n",
        "# Add nodes\n",
        "builder.add_node(\"job_extraction\", job_extraction_node)\n",
        "builder.add_node(\"cv_section_extraction\", cv_section_extractor_node)\n",
        "builder.add_node(\"job_skills_extraction\", job_skills_extractor_node)\n",
        "builder.add_node(\"cv_skills_extraction\", cv_skills_extractor_node)\n",
        "builder.add_node(\"skill_matching\", skill_matching_node)\n",
        "builder.add_node(\"history_check\", history_node)\n",
        "builder.add_node(\"cover_writer\", writer_node)\n",
        "\n",
        "# Define the workflow\n",
        "builder.add_edge(START, \"job_extraction\")\n",
        "builder.add_edge(\"job_extraction\", \"cv_section_extraction\")\n",
        "builder.add_edge(\"cv_section_extraction\", \"job_skills_extraction\")\n",
        "builder.add_edge(\"job_skills_extraction\", \"cv_skills_extraction\")\n",
        "builder.add_edge(\"cv_skills_extraction\", \"skill_matching\")\n",
        "builder.add_edge(\"skill_matching\", \"history_check\")\n",
        "builder.add_edge(\"history_check\", \"cover_writer\")\n",
        "builder.add_edge(\"cover_writer\", END)\n",
        "\n",
        "# Compile the graph\n",
        "workflow = builder.compile()\n",
        "print(\"LangGraph workflow compiled successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title MAIN PIPELINE\n",
        "def run_pipeline(cv_file, excel_file, job_text_input):\n",
        "    \"\"\"Main processing pipeline\"\"\"\n",
        "    try:\n",
        "        print(\"ðŸš€ Starting pipeline...\")\n",
        "\n",
        "        # Initialize state\n",
        "        initial_state = State()\n",
        "\n",
        "        # Process CV file\n",
        "        if cv_file:\n",
        "            cv_path = cv_file.name\n",
        "            cv_text, cv_filename = FileProcessor.extract_text(cv_path)\n",
        "            initial_state['cv_text'] = cv_text\n",
        "            initial_state['candidate_name'] = cv_filename\n",
        "            print(f\"ðŸ“„ CV processed: {len(cv_text)} characters from {cv_filename}\")\n",
        "        else:\n",
        "            print(\"âš ï¸ No CV file provided\")\n",
        "            return \"Error: No CV file provided\", \"\", \"\", None\n",
        "\n",
        "        # Process Excel file\n",
        "        if excel_file:\n",
        "            excel_path = excel_file.name\n",
        "            df = FileProcessor.load_excel_data(excel_path)\n",
        "            initial_state['applications_df'] = df\n",
        "            print(f\"ðŸ“Š Excel processed: {len(df)} rows\")\n",
        "        else:\n",
        "            initial_state['applications_df'] = pd.DataFrame()\n",
        "            print(\"âš ï¸ No Excel file provided\")\n",
        "\n",
        "        # Set job text\n",
        "        initial_state['job_text'] = job_text_input.strip()\n",
        "        print(f\"ðŸ“ Job text: {len(job_text_input)} characters\")\n",
        "\n",
        "        if not job_text_input.strip():\n",
        "            return \"Error: No job description provided\", \"\", \"\", None\n",
        "\n",
        "        # Run the workflow\n",
        "        print(\"ðŸ”„ Running LangGraph workflow...\")\n",
        "        final_state = workflow.invoke(initial_state)\n",
        "\n",
        "        # Extract results with error handling\n",
        "        company = final_state.get('company', 'Unknown')\n",
        "        title = final_state.get('title', 'Unknown')\n",
        "        cv_filename = final_state.get('candidate_name', 'Unknown')\n",
        "        overall_skill_match = final_state.get('overall_skill_match', 0)\n",
        "        history_note = final_state.get('history_note', 'No history checked')\n",
        "\n",
        "        # Safe extraction of skills data\n",
        "        def safe_extract_skills(skill_data, field_name):\n",
        "            \"\"\"Safely extract skill names from various data formats\"\"\"\n",
        "            print(f\"DEBUG - Processing {field_name}: type={type(skill_data)}, length={len(skill_data) if skill_data else 0}\")\n",
        "\n",
        "            if not skill_data:\n",
        "                return []\n",
        "\n",
        "            extracted = []\n",
        "            for i, item in enumerate(skill_data):\n",
        "                print(f\"DEBUG - {field_name}[{i}]: type={type(item)}, value={item}\")\n",
        "\n",
        "                if isinstance(item, dict):\n",
        "                    # If it's a dict, look for common key names\n",
        "                    skill_name = item.get('skill', item.get('name', str(item)))\n",
        "                    extracted.append(skill_name)\n",
        "                elif isinstance(item, str):\n",
        "                    extracted.append(item)\n",
        "                else:\n",
        "                    extracted.append(str(item))\n",
        "\n",
        "            print(f\"DEBUG - {field_name} extracted: {extracted[:3]}\")\n",
        "            return extracted\n",
        "\n",
        "        # Extract skill lists safely\n",
        "        matched_skills = safe_extract_skills(final_state.get('matched_skills', []), 'matched')\n",
        "        unmatched_skills = safe_extract_skills(final_state.get('unmatched_skills', []), 'unmatched')\n",
        "        cv_skills_raw = final_state.get('cv_skills', [])\n",
        "        job_skills_raw = final_state.get('job_skills', [])\n",
        "\n",
        "        # Ensure cv_skills and job_skills are lists of strings\n",
        "        cv_skills = [str(skill).strip() for skill in cv_skills_raw] if cv_skills_raw else []\n",
        "        job_skills = [str(skill).strip() for skill in job_skills_raw] if job_skills_raw else []\n",
        "\n",
        "        print(f\"DEBUG - Final cv_skills: {cv_skills[:3]}\")\n",
        "        print(f\"DEBUG - Final job_skills: {job_skills[:3]}\")\n",
        "        print(f\"DEBUG - Final matched_skills: {matched_skills[:3]}\")\n",
        "        print(f\"DEBUG - Final unmatched_skills: {unmatched_skills[:3]}\")\n",
        "\n",
        "        cover_letter = final_state.get('cover_letter', 'No cover letter generated')\n",
        "\n",
        "        # Extract candidate name from CV\n",
        "        try:\n",
        "            candidate_name = extract_candidate_info(final_state.get('cv_text', ''))\n",
        "        except:\n",
        "            candidate_name = \"Name not found\"\n",
        "\n",
        "        # Create summary\n",
        "        summary = f\"\"\"ðŸ“‹ APPLICATION SUMMARY\n",
        "\n",
        "ðŸ“„ CV File: {cv_filename}\n",
        "ðŸ‘¤ Candidate: {candidate_name}\n",
        "ðŸ¢ Company: {company}\n",
        "ðŸ’¼ Position: {title}\n",
        "ðŸ“Š Skills Match: {overall_skill_match}%\n",
        "\n",
        "ðŸ“ˆ Application History:\n",
        "{history_note}\n",
        "\n",
        "ðŸŽ¯ Ready for application submission!\"\"\"\n",
        "\n",
        "        # Create skills summary with safe joining\n",
        "        try:\n",
        "            skills_summary = f\"\"\"ðŸŽ¯ SKILLS ANALYSIS\n",
        "\n",
        "âœ… YOUR SKILLS ({len(cv_skills)}):\n",
        "{', '.join(cv_skills) if cv_skills else 'None extracted'}\n",
        "\n",
        "ðŸŽ¯ JOB REQUIREMENTS ({len(job_skills)}):\n",
        "{', '.join(job_skills) if job_skills else 'None extracted'}\n",
        "\n",
        "ðŸ’ª MATCHED SKILLS ({len(matched_skills)}):\n",
        "{', '.join(matched_skills) if matched_skills else 'None matched'}\n",
        "\n",
        "âš ï¸ SKILLS TO DEVELOP ({len(unmatched_skills)}):\n",
        "{', '.join(unmatched_skills[:20]) if unmatched_skills else 'None identified'}\n",
        "\n",
        "ðŸ“Š OVERALL MATCH: {overall_skill_match}% ({len(matched_skills)}/{len(job_skills)} skills)\"\"\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR creating skills_summary: {e}\")\n",
        "            skills_summary = f\"Error creating skills summary: {str(e)}\"\n",
        "\n",
        "        print(\"âœ… Pipeline completed successfully!\")\n",
        "\n",
        "        return summary, skills_summary, cover_letter, None\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"âŒ Pipeline error: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        import traceback\n",
        "        print(\"Full traceback:\")\n",
        "        traceback.print_exc()\n",
        "        return error_msg, \"\", \"\", None\n",
        "\n",
        "# Helper function for candidate name extraction\n",
        "def extract_candidate_info(cv_text: str) -> str:\n",
        "    \"\"\"Extract candidate name and title from CV text\"\"\"\n",
        "    if not cv_text:\n",
        "        return \"Name not found\"\n",
        "\n",
        "    system = \"Extract the candidate's full name and professional title from this CV. Return in format: 'Name - Title'\"\n",
        "    prompt = f\"Extract name and title from this CV:\\n{cv_text[:1000]}\"\n",
        "\n",
        "    try:\n",
        "        response = llm.generate(prompt, system_prompt=system, max_tokens=100)\n",
        "        return response.strip()\n",
        "    except:\n",
        "        # Fallback: look for name patterns in first few lines\n",
        "        lines = cv_text.split('\\n')[:5]\n",
        "        for line in lines:\n",
        "            if len(line.strip()) > 5 and len(line.strip()) < 50:\n",
        "                return line.strip()\n",
        "        return \"Name not found\""
      ],
      "metadata": {
        "id": "1YJFrJ0OxSt8"
      },
      "id": "1YJFrJ0OxSt8",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **INTERFACE**"
      ],
      "metadata": {
        "id": "sKxYrpN5w8ZA"
      },
      "id": "sKxYrpN5w8ZA"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ab966ea2",
      "metadata": {
        "id": "ab966ea2"
      },
      "outputs": [],
      "source": [
        "# Gradio UI\n",
        "def create_interface():\n",
        "    \"\"\"Create Gradio interface\"\"\"\n",
        "\n",
        "    with gr.Blocks(title=\"ðŸ¤– AI Cover Letter Generator\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # ðŸ¤– AI Cover Letter Generator\n",
        "        ### Powered by LangGraph & Groq API\n",
        "\n",
        "        Upload your CV and job application history, then paste the job description to generate a personalized cover letter.\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                cv_upload = gr.File(\n",
        "                    label=\"ðŸ“„ Upload CV (PDF/DOCX/TXT)\",\n",
        "                    file_types=['.pdf', '.docx', '.txt']\n",
        "                )\n",
        "\n",
        "                excel_upload = gr.File(\n",
        "                    label=\"ðŸ“Š Upload Application History (XLSX)\",\n",
        "                    file_types=['.xlsx', '.xls']\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                job_input = gr.Textbox(\n",
        "                    label=\"ðŸ“ Job Description\",\n",
        "                    placeholder=\"Paste the Company, Position and complete job description here...\",\n",
        "                    lines=12\n",
        "                )\n",
        "\n",
        "        generate_btn = gr.Button(\n",
        "            \"ðŸš€ Generate Cover Letter\",\n",
        "            variant=\"primary\",\n",
        "            size=\"lg\"\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            summary_output = gr.Textbox(\n",
        "                label=\"ðŸ“‹ Summary\",\n",
        "                lines=10,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            skills_output = gr.Textbox(\n",
        "                label=\"ðŸŽ¯ Skills Analysis\",\n",
        "                lines=10,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "        cover_output = gr.Textbox(\n",
        "            label=\"âœï¸ Generated Cover Letter\",\n",
        "            lines=15,\n",
        "            show_copy_button=True\n",
        "        )\n",
        "\n",
        "        download_output = gr.File(\n",
        "            label=\"ðŸ’¾ Download Cover Letter\",\n",
        "            visible=False\n",
        "        )\n",
        "\n",
        "        # Event handler\n",
        "        generate_btn.click(\n",
        "            fn=run_pipeline,\n",
        "            inputs=[cv_upload, excel_upload, job_input],\n",
        "            outputs=[summary_output, skills_output, cover_output, download_output]\n",
        "        )\n",
        "\n",
        "    return demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***MAIN EXECUTION***"
      ],
      "metadata": {
        "id": "HGqW505hOZ3A"
      },
      "id": "HGqW505hOZ3A"
    },
    {
      "cell_type": "code",
      "source": [
        "demo = create_interface()\n",
        "demo.launch(share=True, show_api=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "8WQQtYrUxJsV",
        "outputId": "08caa64d-4d5c-423c-ae09-eb3ae3f2181e"
      },
      "id": "8WQQtYrUxJsV",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4a300e4c6a3bf24033.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4a300e4c6a3bf24033.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ],
  "metadata": {
    "title": "CoverLetter_AI_LangGraph_v005",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}