{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7424ae99",
   "metadata": {
    "id": "7424ae99"
   },
   "source": [
    "\n",
    "# Fine-Tuning LLMs with Hugging Face and PyTorch\n",
    "\n",
    "This notebook demonstrates how to fine-tune a pretrained transformer model using Hugging Face `transformers` and `datasets`, specifically for supervised fine-tuning (SFT) using the `SFTTrainer`.\n",
    "\n",
    "## Overview\n",
    "- Install dependencies\n",
    "- Load pretrained model and tokenizer\n",
    "- Load and tokenize dataset\n",
    "- Configure LoRA for parameter-efficient tuning\n",
    "- Train model using `SFTTrainer`\n",
    "- Generate text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fad02f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fad02f0",
    "outputId": "098e3891-1c39-4667-bcba-86a17463385b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting trl\n",
      "  Downloading trl-0.19.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.8.1)\n",
      "Collecting datasets>=3.0.0 (from trl)\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers>=4.51.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.53.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (2.6.0+cu124)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (0.33.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.15)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.0->trl) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.0->trl) (0.21.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.7.14)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.2)\n",
      "Downloading trl-0.19.1-py3-none-any.whl (376 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, datasets, trl\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.4\n",
      "    Uninstalling datasets-2.14.4:\n",
      "      Successfully uninstalled datasets-2.14.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-4.0.0 fsspec-2025.3.0 trl-0.19.1\n"
     ]
    }
   ],
   "source": [
    "# @title Install Required Libraries\n",
    "\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "!pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ec721bb",
   "metadata": {
    "id": "2ec721bb"
   },
   "outputs": [],
   "source": [
    "# @title Import Required Libraries\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from transformers import BitsAndBytesConfig, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "piRVlM1ufb4X",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "piRVlM1ufb4X",
    "outputId": "8643fe0f-6a68-4928-b282-e502eac2c60d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# @title Select the device\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6be6428",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362,
     "referenced_widgets": [
      "8ee47dba39234a9fae28803feeaa4c51",
      "747306eaba314177ad303e3fb9503e5b",
      "2667a56f44db470da8b92ede156c0497",
      "2d5016d6a8c4403a991ef0fd25ec15dc",
      "5129c42a72d8464cbe4a26f30448655a",
      "45b329cb4e00449b98c704f56c2f72f7",
      "a1b3dbdad11c49d18cb55df784b174e7",
      "718673b5180e4cb1ba0f6eb44e73eee6",
      "54e4c3a7b3c0434aa265bb472ad584f0",
      "9589877f68d4472a932e39243b9a9d37",
      "bfd2de75cafe417785f77ccd8e953673",
      "2370f569a00c413e8dccf38ee7ec1070",
      "1cf905d4c3314e7990a66c1e72b34535",
      "4176b9dfc73f4651a60f82fb668206c9",
      "fcc57afa6e6b4cbd9fed064e7e4424d0",
      "c7d47eb647964772b865e85507f233b6",
      "1825556dafc5425b89904932a3d1d67c",
      "5b13562b236648dfb751af013473de2a",
      "d89761d68528425b885900e08b9a4dac",
      "7c0b9650ac0343b3938a4abf1c457da6",
      "a135fd227c7b453eb05425afa9d91063",
      "f4cfb3b1130a429ebf410c89226016e7",
      "ad4ef461cf894bfa933f452c6729dc8f",
      "b5ab6a46a2b048c89c361ec3aea18d41",
      "b8b461d9182942d1b5c70b05a43c2d02",
      "92b4351cd7ce49f5b87d974ed2362770",
      "2824b8b612344b41b82a763944d6af11",
      "31e0ab33f9cd4f618fd847fad109b94e",
      "28a21e76293a42aca4bb49beecf91ab1",
      "0bea8e07a8df4e58a7d5f945a426e5e8",
      "0460acde424441168f531b9fa8109651",
      "1227800e13b9417892f62557c778bdc4",
      "9b724e56c33f4977a28f78d8d1d7b998",
      "9ec2140468ac4d5f8d5a9695d14f93e4",
      "cc4e03d2069d4edbb6517d5276fc6f78",
      "81fbf521a9e54cd195ed670ec0cbdde4",
      "a91ffeebbed340ceb717c2ebbad04645",
      "5003c838c5e34ebc869221f6375cd644",
      "7df070e30ee84a61a3312f5d8714f6e4",
      "c3940f1de5604d9aa1c4c8844674d9d6",
      "f10d0eaed06b42e1833186c0e978f402",
      "c23b3f95b3c849fa8b0d0b558a03dbc5",
      "9e7b4638ad764663945cb3c081ab8856",
      "e6ad8bceb02c4e5192c2176f73fd1880",
      "a9a5694aee8c43018967006eaea88420",
      "a646f658ebef4985a6710489b5773d67",
      "1a049c1ea2df42318fd7af7825be037c",
      "e71b37e5c0414000a0ce746d1b7f1335",
      "34b00a3b1b6c43649b8f3642815ee689",
      "746ab4c9a42f44f0aed25cf2a4f80090",
      "89b8d778db43496baf6f65ecc840a7dc",
      "5359ef350bba4720b4edd3de3d387af7",
      "eac5a30edc994959883e08c2c8740ed2",
      "9cf7b667ee74423da081688fa4531ed2",
      "e82cc859b45349e99ac598664802dbeb",
      "dfc5e330148741588984b1bd33d0a8ec",
      "a3a383f898e0483586a115c13958e320",
      "ad1c211f836e42cb85c39ec051187191",
      "4276e7e7f2df430d95e77da1807f8abe",
      "0c7887ab8669422b917c2664c9ef119e",
      "edb9efb55198414ab8c9754bf36a309f",
      "3169172f04bb421ea5f9cac87683a0c5",
      "5e29cb6ddf814646a83c9c85a4b05600",
      "84d2af5933944358a3cba7c27681f138",
      "dd916f8c4fc04592a9b25072dbd439f5",
      "c795a4efda364edea784697c32c5bb5c",
      "614b0d319d7042f7931fdf83a3b63987",
      "43ace9c157904ac1847922a0e88632ba",
      "b243a50e15c24ea58a44062b9c74d844",
      "e7f465876b2941918f8d8fd6ab7f0a78",
      "791771372c014ceaae9ec92a938a3f07",
      "4e3fe3c2e2434733a355513f046094c2",
      "2ee941a67a414952b5887b3e045c431f",
      "03023de512a645e88c0fe59db305e3a6",
      "2cf6e221e1de46748c1c4171926ead89",
      "816264e153654ebea1c157c39d0f717a",
      "84ceed2aa4384c7b970ffdbd0d0b5a7f",
      "386fc98e734a44309d29422a5d4d1189",
      "0e76686a164b400f9775d301585cccbe",
      "1cc163a3d0a44e17b6c4b8c0e001a10b",
      "4d9f1808e5dd47d8855bb8a19e7e184d",
      "53f95183c8fa46c5b94a222981600fa1",
      "9821e16c9e364d5ba5169fbe12c1fdaa",
      "4f4117805b414a8ea7f3cb6fd715c80d",
      "12e79cf2246442d18fbff2de3c0bfa3a",
      "5084c49d3ac440feb7d5818122cfec29",
      "a48652ee415f4f21b75e0e45804aa639",
      "9e8dddb4781e4c42b564058678f7b6ab"
     ]
    },
    "id": "e6be6428",
    "outputId": "409c51e6-86d1-431e-b2e0-6c87657a0545"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee47dba39234a9fae28803feeaa4c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2370f569a00c413e8dccf38ee7ec1070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4ef461cf894bfa933f452c6729dc8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec2140468ac4d5f8d5a9695d14f93e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a5694aee8c43018967006eaea88420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc5e330148741588984b1bd33d0a8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614b0d319d7042f7931fdf83a3b63987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386fc98e734a44309d29422a5d4d1189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Load Pretrained Model and Tokenizer\n",
    "\n",
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "uUUN3mLnaa8B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUUN3mLnaa8B",
    "outputId": "43f6eb20-f62f-4a74-c9bc-ced8a0b629b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Pretrained Model Output:\n",
      "What is Beriberi?\n",
      "\n",
      "Beriberi is a small, medium-sized, and very popular coffee shop in the city of Beriberi, in the city of Cagliari, in the province of Cagliari, in the province of Cag\n"
     ]
    }
   ],
   "source": [
    "# @title Test the model before fine-tuning to illustrate baseline performance\n",
    "prompt = \"What is Beriberi ?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "print(\"ğŸ” Pretrained Model Output:\")\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9314051e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399,
     "referenced_widgets": [
      "eab863b2b24f4c69a4e3ae94b64cb4b3",
      "d1f3e7af65d74d6a9160f2862af95e0b",
      "8a00f5c9b94a438789c0c052c466cab3",
      "d1905e3dfc39450ca79cf43a69e3cc85",
      "43558a032263413b8cf32f15f21f3cef",
      "741296d0e0d74dfda04147cd34e0a22d",
      "b6aefc2d644a4cd89cdab68c9fee09d2",
      "5902996f87cc4070b1b896520ffa4e82",
      "b5b9079336a24ffd948136fe6baf7649",
      "8c01a270879449e4b49fe597edf60f10",
      "b6ebdf6b903b40b9b30a89cd6babc010",
      "eec2a350ef35465ea1cfff86c8e586b6",
      "b229645949e94bb48b430ef707c81c4f",
      "71fbc019886d4ed0b3f8f7292d796b69",
      "29b4666b0fae40aeb6f10f7b7b9bfe24",
      "131f980e536246aa8ba8788c3d3524a5",
      "2526eef4009545e3b2ebecd2a3c43202",
      "99c115f079144d58998a054a400330f5",
      "961356271c35444e9878588b71550fe6",
      "3bfcc9cdeecf4c4cb2e8420a5c697d54",
      "bb163612a34845748e2d07c88c58d95f",
      "7744cd7f22754e65893647f47b283cc4",
      "806787bd32094977b6820f4749fd7240",
      "b4cad2276e3e4b1db4ff62ca0da74955",
      "6874b4200d85439e9037c8ae39042517",
      "69ed249ad3184a7da10e75924ab1aa83",
      "12218c74c27747a585ebb8be03d48de9",
      "7dd0541d1f174feca3f8078b032be5db",
      "a1f1a03d4134462383c244066905fd9e",
      "1d19e36963b347438bc2d0c1820c358f",
      "64784fac7d2d48f5bc7fa05bcf936903",
      "832a0c0965ed47098fc4878aaedc7e02",
      "3f30927b1df643939311195536bee030"
     ]
    },
    "id": "9314051e",
    "outputId": "a16e4392-bff3-4987-810f-3ec40d381866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying medical_dialog dataset...\n",
      "Attempting to load lavita/MedQuAD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab863b2b24f4c69a4e3ae94b64cb4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec2a350ef35465ea1cfff86c8e586b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)-00000-of-00001-e36383d177026d53.parquet:   0%|          | 0.00/10.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806787bd32094977b6820f4749fd7240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/47441 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded!\n",
      "Dataset structure after splitting:\n",
      "Dataset({\n",
      "    features: ['document_id', 'document_source', 'document_url', 'category', 'umls_cui', 'umls_semantic_types', 'umls_semantic_group', 'synonyms', 'question_id', 'question_focus', 'question_type', 'question', 'answer'],\n",
      "    num_rows: 47441\n",
      "})\n",
      "\n",
      "First example from the training dataset:\n",
      "{'document_id': '0000559', 'document_source': 'GHR', 'document_url': 'https://ghr.nlm.nih.gov/condition/keratoderma-with-woolly-hair', 'category': None, 'umls_cui': 'C0343073', 'umls_semantic_types': 'T047', 'umls_semantic_group': 'Disorders', 'synonyms': 'KWWH', 'question_id': '0000559-1', 'question_focus': 'keratoderma with woolly hair', 'question_type': 'information', 'question': 'What is (are) keratoderma with woolly hair ?', 'answer': 'Keratoderma with woolly hair is a group of related conditions that affect the skin and hair and in many cases increase the risk of potentially life-threatening heart problems. People with these conditions have hair that is unusually coarse, dry, fine, and tightly curled. In some cases, the hair is also sparse. The woolly hair texture typically affects only scalp hair and is present from birth. Starting early in life, affected individuals also develop palmoplantar keratoderma, a condition that causes skin on the palms of the hands and the soles of the feet to become thick, scaly, and calloused.  Cardiomyopathy, which is a disease of the heart muscle, is a life-threatening health problem that can develop in people with keratoderma with woolly hair. Unlike the other features of this condition, signs and symptoms of cardiomyopathy may not appear until adolescence or later. Complications of cardiomyopathy can include an abnormal heartbeat (arrhythmia), heart failure, and sudden death.  Keratoderma with woolly hair comprises several related conditions with overlapping signs and symptoms. Researchers have recently proposed classifying keratoderma with woolly hair into four types, based on the underlying genetic cause. Type I, also known as Naxos disease, is characterized by palmoplantar keratoderma, woolly hair, and a form of cardiomyopathy called arrhythmogenic right ventricular cardiomyopathy (ARVC). Type II, also known as Carvajal syndrome, has hair and skin abnormalities similar to type I but features a different form of cardiomyopathy, called dilated left ventricular cardiomyopathy. Type III also has signs and symptoms similar to those of type I, including ARVC, although the hair and skin abnormalities are often milder. Type IV is characterized by palmoplantar keratoderma and woolly and sparse hair, as well as abnormal fingernails and toenails. Type IV does not appear to cause cardiomyopathy.'}\n",
      "\n",
      "Features of the dataset:\n",
      "{'document_id': Value('string'), 'document_source': Value('string'), 'document_url': Value('string'), 'category': Value('string'), 'umls_cui': Value('string'), 'umls_semantic_types': Value('string'), 'umls_semantic_group': Value('string'), 'synonyms': Value('string'), 'question_id': Value('string'), 'question_focus': Value('string'), 'question_type': Value('string'), 'question': Value('string'), 'answer': Value('string')}\n"
     ]
    }
   ],
   "source": [
    "# @title Load Dataset\n",
    "\n",
    "# Load a medical dialogue or medical Q&A dataset.\n",
    "print(\"\\nTrying medical_dialog dataset...\")\n",
    "\n",
    "print(\"Attempting to load lavita/MedQuAD...\")\n",
    "try:\n",
    "    dataset = load_dataset(\"lavita/MedQuAD\", split=\"train\")\n",
    "    print(\"Successfully loaded!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "\n",
    "# Print the structure of the dataset and show one example from the training set\n",
    "print(\"Dataset structure after splitting:\")\n",
    "print(dataset)\n",
    "print(\"\\nFirst example from the training dataset:\")\n",
    "print(dataset[0])\n",
    "\n",
    "# You can also inspect the features (columns) of the dataset\n",
    "print(\"\\nFeatures of the dataset:\")\n",
    "print(dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "X9cGObYhrgWl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "141af68bf6124416996d05e2c398b656",
      "fb898ddf0b57492798c93148d509a9d3",
      "c41b6b9886bf400589778e435482eb78",
      "7d83e5d112874e1dac1d9b1a9f375254",
      "ff9e7a4ca7c44cff81ceb0e073eb70be",
      "8c6241aa805a45df871e181061284037",
      "19a91a1d58424bba8826a4eae54864bc",
      "c973d73de4364f738d21c45317422824",
      "09f2f7cde2604bdd8084aa6c26f3ddbf",
      "6b7eed2188164d52ab713c4c4f815e42",
      "d7c76ca431e04ff49de3f616e1fca696",
      "32706f34662f4439ad7be41ab221a0ab",
      "e8b623b9a3594a40bdb67d7c749dfa7b",
      "5cfaa94ed45f401088475f20090dba1c",
      "396bb264ee144c9f93dc2246cf44e1aa",
      "9cd0650eeee64a20a549b646125a575a",
      "5da4f1ad76cc4e5681745551a894419f",
      "0da8389942004a848f1c9327bd67da06",
      "d7c216ad87544ee08a9e2e078a882dfe",
      "1fa5c45d235849ca816c309db8bf3402",
      "10ec894e99f14122be5bfc6ed79a23af",
      "a0b12c6fcf6f4c35b9ef3c92a98fed5e"
     ]
    },
    "id": "X9cGObYhrgWl",
    "outputId": "7fdd979b-38e6-4b81-983e-68835e008ced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STARTING DATASET CLEANING ===\n",
      "Original dataset size: 47441\n",
      "\n",
      "=== FILTERING OUT MISSING ANSWERS ===\n",
      "Examples with valid answers: 16407\n",
      "Examples with missing answers: 31034\n",
      "Kept 34.6% of original data\n",
      "\n",
      "=== CREATING CLEANED DATASET ===\n",
      "âœ… Cleaned dataset created successfully!\n",
      "Size: 16407 examples\n",
      "\n",
      "=== SAMPLE FROM CLEANED DATASET ===\n",
      "Question: What is (are) keratoderma with woolly hair ?\n",
      "Answer: Keratoderma with woolly hair is a group of related conditions that affect the skin and hair and in many cases increase the risk of potentially life-threatening heart problems. People with these conditions have hair that is unusually coarse, dry, fine, and tightly curled. In some cases, the hair is a...\n",
      "Category: None\n",
      "Question Type: information\n",
      "Source: GHR\n",
      "\n",
      "=== CREATING TRAIN/TEST SPLIT ===\n",
      "Training examples: 14766\n",
      "Evaluation examples: 1641\n",
      "\n",
      "=== CREATING SIMPLIFIED VERSION ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141af68bf6124416996d05e2c398b656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32706f34662f4439ad7be41ab221a0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified datasets created!\n",
      "Training features: ['document_id', 'document_source', 'document_url', 'category', 'umls_cui', 'umls_semantic_types', 'umls_semantic_group', 'synonyms', 'question_id', 'question_focus', 'question_type', 'question', 'answer']\n",
      "\n",
      "=== CLEANED DATASET STATISTICS ===\n",
      "Top question types:\n",
      "  information: 4082\n",
      "  symptoms: 2507\n",
      "  treatment: 2180\n",
      "  inheritance: 1304\n",
      "  frequency: 1004\n",
      "  genetic changes: 981\n",
      "  causes: 651\n",
      "  exams and tests: 595\n",
      "  research: 351\n",
      "  outlook: 314\n",
      "\n",
      "Category distribution:\n",
      "  None: 13874\n",
      "  Disease: 623\n",
      "  Other: 269\n",
      "\n",
      "=== CREATING FINAL DATASET DICTIONARY ===\n",
      "Final datasets created:\n",
      "- Training: 14766 examples\n",
      "- Testing: 1641 examples\n",
      "\n",
      "=== SAMPLE QUESTIONS BY TYPE ===\n",
      "\n",
      "SYMPTOMS:\n",
      "Q: What are the symptoms of Mental retardation-hypotonic facies syndrome X-linked, 1 ?\n",
      "A: What are the signs and symptoms of Mental retardation-hypotonic facies syndrome X-linked, 1? The Human Phenotype Ontology provides the following list of signs and symptoms for Mental retardation-hypot...\n",
      "\n",
      "INFORMATION:\n",
      "Q: What is (are) Beriberi ?\n",
      "A: Beriberi is a condition that occurs in people who are deficient in thiamine (vitamin B1). There are two major types of beriberi: wet beriberi which affects the cardiovascular system and dry beriberi w...\n",
      "\n",
      "FREQUENCY:\n",
      "Q: How many people are affected by hystrix-like ichthyosis with deafness ?\n",
      "A: HID is a rare disorder. Its prevalence is unknown....\n",
      "\n",
      "INHERITANCE:\n",
      "Q: Is hereditary xanthinuria inherited ?\n",
      "A: This condition is inherited in an autosomal recessive pattern, which means both copies of the gene in each cell have mutations. The parents of an individual with an autosomal recessive condition each ...\n",
      "\n",
      "EXAMS AND TESTS:\n",
      "Q: How to diagnose Jejunal atresia ?\n",
      "A: How is jejunal atresia diagnosed? In some cases, jejunal atresia may be diagnosed before birth on a prenatal ultrasound. After birth, a diagnosis is often suspected based on the presence of characteri...\n",
      "\n",
      "=== READY FOR TRAINING ===\n",
      "SUCCESS! Your medical Q&A dataset is ready!\n",
      "Dataset Summary:\n",
      "   - Original size: 47,441 examples\n",
      "   - Cleaned size: 16,407 examples\n",
      "   - Training set: 14,766 examples\n",
      "   - Test set: 1,641 examples\n",
      "   - Data retention: 34.6%\n",
      "\n",
      "Usage:\n",
      "   - Use 'final_datasets['train']' for training\n",
      "   - Use 'final_datasets['test']' for evaluation\n",
      "   - Each example has: question, answer, category, question_type, document_source\n",
      "\n",
      "Dataset focuses on:\n",
      "   - Medical conditions and genetic disorders\n",
      "   - Treatment information\n",
      "   - Symptoms and diagnosis\n",
      "   - Inheritance patterns\n",
      "   - Genetic changes and causes\n"
     ]
    }
   ],
   "source": [
    "# @title Dataset Cleaning\n",
    "\n",
    "print(\"=== STARTING DATASET CLEANING ===\")\n",
    "print(f\"Original dataset size: {len(dataset)}\")\n",
    "\n",
    "# Step 1: Filter out examples with missing answers\n",
    "print(\"\\n=== FILTERING OUT MISSING ANSWERS ===\")\n",
    "filtered_examples = []\n",
    "missing_count = 0\n",
    "\n",
    "for example in dataset:\n",
    "    if example['answer'] is not None and example['answer'].strip() != '':\n",
    "        filtered_examples.append(example)\n",
    "    else:\n",
    "        missing_count += 1\n",
    "\n",
    "print(f\"Examples with valid answers: {len(filtered_examples)}\")\n",
    "print(f\"Examples with missing answers: {missing_count}\")\n",
    "print(f\"Kept {len(filtered_examples)/len(dataset)*100:.1f}% of original data\")\n",
    "\n",
    "# Step 2: Convert to pandas DataFrame and then back to Dataset\n",
    "print(\"\\n=== CREATING CLEANED DATASET ===\")\n",
    "df = pd.DataFrame(filtered_examples)\n",
    "cleaned_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "print(\"âœ… Cleaned dataset created successfully!\")\n",
    "print(f\"Size: {len(cleaned_dataset)} examples\")\n",
    "\n",
    "# Step 3: Show sample from cleaned dataset\n",
    "print(\"\\n=== SAMPLE FROM CLEANED DATASET ===\")\n",
    "sample = cleaned_dataset[0]\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"Answer: {sample['answer'][:300]}...\")\n",
    "print(f\"Category: {sample['category']}\")\n",
    "print(f\"Question Type: {sample['question_type']}\")\n",
    "print(f\"Source: {sample['document_source']}\")\n",
    "\n",
    "# Step 4: Create train/test split\n",
    "print(\"\\n=== CREATING TRAIN/TEST SPLIT ===\")\n",
    "split_datasets = cleaned_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_datasets[\"train\"]\n",
    "eval_dataset = split_datasets[\"test\"]\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Evaluation examples: {len(eval_dataset)}\")\n",
    "\n",
    "# Step 5: Create simplified version with essential fields only\n",
    "print(\"\\n=== CREATING SIMPLIFIED VERSION ===\")\n",
    "def simplify_example(example):\n",
    "    return {\n",
    "        'question': example['question'],\n",
    "        'answer': example['answer']\n",
    "    }\n",
    "\n",
    "simplified_train = train_dataset.map(simplify_example)\n",
    "simplified_eval = eval_dataset.map(simplify_example)\n",
    "\n",
    "print(\"Simplified datasets created!\")\n",
    "print(f\"Training features: {list(simplified_train.features.keys())}\")\n",
    "\n",
    "# Step 6: Show statistics about the cleaned data\n",
    "print(\"\\n=== CLEANED DATASET STATISTICS ===\")\n",
    "\n",
    "# Count question types\n",
    "question_types = {}\n",
    "categories = {}\n",
    "\n",
    "for example in train_dataset:\n",
    "    qtype = example['question_type']\n",
    "    question_types[qtype] = question_types.get(qtype, 0) + 1\n",
    "\n",
    "    cat = example['category']\n",
    "    categories[cat] = categories.get(cat, 0) + 1\n",
    "\n",
    "print(\"Top question types:\")\n",
    "for qtype, count in sorted(question_types.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"  {qtype}: {count}\")\n",
    "\n",
    "print(\"\\nCategory distribution:\")\n",
    "for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {cat}: {count}\")\n",
    "\n",
    "# Step 7: Create final dataset dictionary\n",
    "print(\"\\n=== CREATING FINAL DATASET DICTIONARY ===\")\n",
    "final_datasets = DatasetDict({\n",
    "    'train': simplified_train,\n",
    "    'test': simplified_eval\n",
    "})\n",
    "\n",
    "print(f\"Final datasets created:\")\n",
    "print(f\"- Training: {len(final_datasets['train'])} examples\")\n",
    "print(f\"- Testing: {len(final_datasets['test'])} examples\")\n",
    "\n",
    "# Step 8: Show sample questions from different types\n",
    "print(\"\\n=== SAMPLE QUESTIONS BY TYPE ===\")\n",
    "seen_types = set()\n",
    "for example in train_dataset:\n",
    "    qtype = example['question_type']\n",
    "    if qtype not in seen_types and len(seen_types) < 5:\n",
    "        print(f\"\\n{qtype.upper()}:\")\n",
    "        print(f\"Q: {example['question']}\")\n",
    "        print(f\"A: {example['answer'][:200]}...\")\n",
    "        seen_types.add(qtype)\n",
    "\n",
    "# Step 9: Export information for training\n",
    "print(\"\\n=== READY FOR TRAINING ===\")\n",
    "print(\"SUCCESS! Your medical Q&A dataset is ready!\")\n",
    "print(f\"Dataset Summary:\")\n",
    "print(f\"   - Original size: {len(dataset):,} examples\")\n",
    "print(f\"   - Cleaned size: {len(cleaned_dataset):,} examples\")\n",
    "print(f\"   - Training set: {len(final_datasets['train']):,} examples\")\n",
    "print(f\"   - Test set: {len(final_datasets['test']):,} examples\")\n",
    "print(f\"   - Data retention: {len(cleaned_dataset)/len(dataset)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nUsage:\")\n",
    "print(\"   - Use 'final_datasets['train']' for training\")\n",
    "print(\"   - Use 'final_datasets['test']' for evaluation\")\n",
    "print(\"   - Each example has: question, answer, category, question_type, document_source\")\n",
    "\n",
    "print(f\"\\nDataset focuses on:\")\n",
    "print(\"   - Medical conditions and genetic disorders\")\n",
    "print(\"   - Treatment information\")\n",
    "print(\"   - Symptoms and diagnosis\")\n",
    "print(\"   - Inheritance patterns\")\n",
    "print(\"   - Genetic changes and causes\")\n",
    "\n",
    "# Variables now available for use:\n",
    "# - dataset (original)\n",
    "# - cleaned_dataset (filtered)\n",
    "# - final_datasets (ready for training)\n",
    "# - train_dataset, eval_dataset (full versions)\n",
    "# - simplified_train, simplified_eval (simplified versions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Jn5uP7zBwZJj",
   "metadata": {
    "id": "Jn5uP7zBwZJj"
   },
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "EWBqdXRVwn3B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225,
     "referenced_widgets": [
      "d8f39f21976b4abd97d969c2b21b732d",
      "527566812fd54cca87bfa9374ed148a7",
      "18d107db1ce84954bfa88ad9689de5a7",
      "1e854df7ad7146989b02198db9f2f14a",
      "49e1223ca23d4e2dad0bef512569802d",
      "2e929f42c703405e946151513eb97a24",
      "4f67eb96643b44fc8c0e8ea971fc434e",
      "ac3a2344d38e4a199d56065ce5460f06",
      "862201ffd7f64089b405c64a3518b3ab",
      "6ba35b35d89141d0bddfc636b38dbe8b",
      "50489fe9c57642e296770da1c2b4e410",
      "a102de4ff88c4b6bb05955886eb8e76f",
      "fc3e64e0e29e413590acc0355863c06b",
      "4d8242a4c5004edaa3088bbf2a84858c",
      "354055abfb46419ea03834fa3065a481",
      "5c5eef6ed6d14ee9a9717567766555b2",
      "db8ad12ca97742f8805cecda44408f55",
      "27b80a5d8a284fa7afacaac27f13e36e",
      "1f5f4b5751764732bbed79f0b052319f",
      "7b47edbbff7c47b4b103d13b2ae9c41d",
      "e37f5e70dbcf4adfae6fb2b5ca15fcd4",
      "0ead6856ef9142cdb49b669adc64aa4d"
     ]
    },
    "id": "EWBqdXRVwn3B",
    "outputId": "5be751ef-4a68-41e1-f767-be2a28de2cc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting Q&A pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f39f21976b4abd97d969c2b21b732d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a102de4ff88c4b6bb05955886eb8e76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample formatted text:\n",
      "Question: What are the symptoms of Mental retardation-hypotonic facies syndrome X-linked, 1 ?\n",
      "Answer: What are the signs and symptoms of Mental retardation-hypotonic facies syndrome X-linked, 1? The Human Phenotype Ontology provides the following list of signs and symptoms for Mental retardation-hypotonic facies syndrome X-linked, 1. If the information is available, the table below includes how often the symptom is seen in people with this condition. You can use the MedlinePlus Medical Dictionary to look up the definitions for these medical terms. Signs and Symptoms Approximate number of patients (when available) Abnormality of the palate 90% Anteverted nares 90% Cognitive impairment 90% Depressed nasal bridge 90% Microcephaly 90% Narrow forehead 90% Short stature 90% Tented upper lip vermilion 90% Behavioral abnormality 50% Genu valgum 50% Neurological speech impairment 50% Obesity 50% Seizures 35% Abnormality of the hip bone 7.5% Camptodactyly of finger 7.5% Cryptorchidism 7.5% Low posterior hairline 7.5% Wide mouth 7.5% Abnormality of blood and blood-forming tissues - Brachydactyly syndrome - Coarse facial features - Constipation - Decreased testicular size - Delayed skeletal maturation - Dolichocephaly - Drooling - Epicanthus - Exotropia - Gastroesophageal reflux - High palate - Hyperactivity - Hyperreflexia - Hypertelorism - Hypogonadism - Hypoplasia of midface - Hypospadias - Infantile muscular hypotonia - Intellectual disability, progressive - Intellectual disability, severe - Kyphoscoliosis - Lower limb hypertonia - Low-set ears - Macroglossia - Malar flattening - Micropenis - Microtia - Open mouth - Optic atrophy - Paroxysmal bursts of laughter - Pes planus - Phenotypic variability - Posteriorly rotated ears - Protruding tongue - Ptosis - Radial deviation of finger - Renal hypoplasia - Scrotal hypoplasia - Sensorineural hearing impairment - Short neck - Short upper lip - Slender finger - Talipes calcaneovalgus - Talipes equinovarus - Tapered finger - Thick lower lip vermilion - Triangular nasal tip - Upslanted palpebral fissure - U-Shaped upper lip vermilion - Vesicoureteral reflux - Vomiting - Wide nasal bridge - Widely-spaced maxillary central incisors - X-linked recessive inheritance - The Human Phenotype Ontology (HPO) has collected information on how often a sign or symptom occurs in a condition. Much of this information comes from Orphanet, a European rare disease database. The frequency of a sign or symptom is usually listed as a rough estimate of the percentage of patients who have that feature. The frequency may also be listed as a fraction. The first number of the fraction is how many people had the symptom, and the second number is the total number of people who were examined in one study. For example, a frequency of 25/25 means that in a study of 25 people all patients were found to have that symptom. Because these frequencies are based on a specific study, the fractions may be different if another group of patients are examined. Sometimes, no information on frequency is available. In these cases, the sign or symptom may be rare or common.<|endoftext|>\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Create formatted text for causal language modeling\n",
    "def format_medical_qa(example):\n",
    "    \"\"\"\n",
    "    Format the question-answer pair for causal language modeling.\n",
    "    We'll use a clear format that the model can learn to follow.\n",
    "    \"\"\"\n",
    "    question = example['question'].strip()\n",
    "    answer = example['answer'].strip()\n",
    "\n",
    "    # Create a formatted text with clear delimiters\n",
    "    formatted_text = f\"Question: {question}\\nAnswer: {answer}{tokenizer.eos_token}\"\n",
    "\n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "print(\"Formatting Q&A pairs...\")\n",
    "formatted_train = final_datasets['train'].map(format_medical_qa)\n",
    "formatted_test = final_datasets['test'].map(format_medical_qa)\n",
    "\n",
    "print(\"Sample formatted text:\")\n",
    "print(formatted_train[0]['text'])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "WCkFS81xVK_M",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296,
     "referenced_widgets": [
      "7585f881a7104ca28628ae319aa269d5",
      "0de32b2a030541e8a4e32ffd10d7474d",
      "609266cf904f447b9ffdede5bfb56214",
      "03f8a5459aea4a26a74ae40fae036969",
      "d4b2caa66f7d4dbfa8c5f151bb32d45f",
      "ae4fe3e7214b458a93ef515f2e012d97",
      "86fe391fd35a4913a1aa61e9086be80d",
      "b2d525da8b7646c0afd8f7dad7b11518",
      "cceac4a5c1944b71a6006a3f5f81c7f7",
      "254eeae478c044fea745f656991f899e",
      "e02a45b78ffd44b9a445a60f69aff60c",
      "21671774488248feb6a7b774bde2f4af",
      "1d28b57cab8944d39699f798b5749a83",
      "eedacddfbbc8486293135e4b36e79d73",
      "9fe2323ef72a4e12bd0c28cd75cf8abe",
      "f1fdc1872ce84dfc96977c0309ee0aae",
      "bfa29492144e493daf23972add8312f1",
      "bd9f2e4a6f3d4c80aff38d8339dfa66e",
      "5960faa9db2544f8a5e0ef91688d9719",
      "8f1a4a24aefc40bf831ad85f07722f0f",
      "c1cb2f5244cb40a285b66b645c5c5731",
      "5a8fc465f5d344ab9ed29217c64572f7"
     ]
    },
    "id": "WCkFS81xVK_M",
    "outputId": "d4c2a550-8aa6-4af8-8c9e-90343186172e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7585f881a7104ca28628ae319aa269d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21671774488248feb6a7b774bde2f4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized training examples: 14766\n",
      "Tokenized test examples: 1641\n",
      "\n",
      "Sample tokenized example:\n",
      "Input IDs shape: 512\n",
      "Labels shape: 512\n",
      "Attention mask shape: 512\n",
      "Sample decoded text (first 100 tokens): Question: What are the symptoms of Mental retardation-hypotonic facies syndrome X-linked, 1?\n",
      "Answer: What are the signs and symptoms of Mental retardation-hypotonic facies syndrome X-linked, 1? The Human Phenotype Ontology provides the following list of signs and symptoms for Mental retardation-hypotonic facies syndrome X-linked, 1. If the information is available, the table below includes how often the symptom is seen in people with this\n",
      "Sample tokenized IDs (first 20): [24361, 25, 1867, 389, 262, 7460, 286, 21235, 42964, 341, 12, 36362, 313, 9229, 1777, 444, 14027, 1395, 12, 25614]\n"
     ]
    }
   ],
   "source": [
    "# @title Tokenization\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize the formatted text for causal language modeling.\n",
    "    For GPT-style models, input_ids and labels are the same (shifted internally).\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,        # Ensure long sequences are cut\n",
    "        padding=False,          # We'll pad later in batches with DataCollator\n",
    "        max_length=512,         # Set your desired maximum sequence length here\n",
    "        return_tensors=None     # Return lists, not tensors\n",
    "    )\n",
    "\n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    # The model will internally shift them for next-token prediction\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "# Apply the tokenization function to your formatted datasets\n",
    "tokenized_train = formatted_train.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    # Remove the original 'text' column, as it's no longer needed after tokenization\n",
    "    remove_columns=formatted_train.column_names\n",
    ")\n",
    "tokenized_test = formatted_test.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=formatted_test.column_names\n",
    ")\n",
    "\n",
    "print(f\"Tokenized training examples: {len(tokenized_train)}\")\n",
    "print(f\"Tokenized test examples: {len(tokenized_test)}\")\n",
    "\n",
    "print(\"\\nSample tokenized example:\")\n",
    "sample = tokenized_train[0]\n",
    "print(f\"Input IDs shape: {len(sample['input_ids'])}\")\n",
    "print(f\"Labels shape: {len(sample['labels'])}\")\n",
    "print(f\"Attention mask shape: {len(sample['attention_mask'])}\")\n",
    "# Decode a portion to verify\n",
    "sample_text = tokenizer.decode(sample['input_ids'][:100], skip_special_tokens=False)\n",
    "print(f\"Sample decoded text (first 100 tokens): {sample_text}\")\n",
    "print(f\"Sample tokenized IDs (first 20): {sample['input_ids'][:20]}\") # Added this for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pGnH6Ydu4UQV",
   "metadata": {
    "id": "pGnH6Ydu4UQV"
   },
   "outputs": [],
   "source": [
    "# @title A function to show model predictions (for testing)\n",
    "\n",
    "def test_model_response(model, tokenizer, question, max_length=100):\n",
    "    \"\"\"Test the model's response to a question with proper attention mask handling\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Move to the same device as the model\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate response with attention mask\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],  # Include attention mask\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Remove the original question from the response\n",
    "    if question in response:\n",
    "        response = response.replace(question, \"\").strip()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "v7U66q5k4id_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7U66q5k4id_",
    "outputId": "afe7f4a2-f259-401a-f574-8c3a2d5f9c3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing model before training...\n",
      "Pre-training response to 'What is hereditary xanthinuria ?':\n",
      "Answer: What is hereditary xanthinuria?\n",
      "Hexanthinuria is a disorder that affects the kidneys and has several different causes. These include protein-rich foods, infections, malnutrition, and other conditions that affect the nervous system. When this condition is present in the first person with the disorder, it is called hereditary xanthinuria. The condition is caused by mutations in the SLC26A1 gene that is located on the X chromosome. This gene provides instructions for making an enzyme called aryl hydrocarbon receptor type 1\n"
     ]
    }
   ],
   "source": [
    "# @title Test the model before training\n",
    "print(\"\\nTesting model before training...\")\n",
    "test_question = \"What is hereditary xanthinuria ?\"\n",
    "pre_training_response = test_model_response(model, tokenizer, test_question)\n",
    "print(f\"Pre-training response to '{test_question}':\")\n",
    "print(f\"Answer: {pre_training_response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pqLlzn5dweMh",
   "metadata": {
    "id": "pqLlzn5dweMh"
   },
   "source": [
    "## FINE-TUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6f70524",
   "metadata": {
    "id": "a6f70524"
   },
   "outputs": [],
   "source": [
    "# @title Config Lora\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.01,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6784102d",
   "metadata": {
    "id": "6784102d"
   },
   "outputs": [],
   "source": [
    "# @title Define the Training Arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sft-model\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=500,\n",
    "    #max_steps=100,\n",
    "    num_train_epochs=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    label_names=[\"labels\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9588d34d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172,
     "referenced_widgets": [
      "c001d7d68c984b5ba823dcbfcc264381",
      "6234e514c0aa4a27bbb9cc13dde4c8d6",
      "f0522a553116464fa3e8327a1097c76a",
      "68be78bfef064e0e9a612cf21a469538",
      "a289aec9f6914d28ad69146aa7890d6d",
      "a9033566160d43a79c0ba6ca4141eaa5",
      "c00a533aa7b84df581aa42b7b52f96cd",
      "194a694721704b8aa653d5688662afc0",
      "40259a565c854e7ea883eb60dfb4cf51",
      "cd3eb857f7f34cb488d7904c6f2a6f6c",
      "c04ec045284042f88dd465cadf097dfe",
      "ee889d9aa7fb4167a412f4840301a780",
      "15526584162a4bd29e239d72e5e68d4d",
      "f39e4672f42845cf8b7c3fc21e3bc6aa",
      "a3b19480d24a42c18e62918823ea4e52",
      "42da8545af3e485aaef9aafb45544b1f",
      "37ec7fbcd4874b9c9952cad2ccb8951b",
      "d507dec82adf4eff90d4c797b42c2ed6",
      "0dafdf8a27c84be4ac0726dae3c25000",
      "b7b46ef8139443fdba5e6f4adf747b52",
      "8d790246d582453e8cef3e183d9acf46",
      "35f37204f4c642c9be70802e39416030"
     ]
    },
    "id": "9588d34d",
    "outputId": "8d4d7f6f-8202-4d6e-991e-8172874d54dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c001d7d68c984b5ba823dcbfcc264381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/14766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee889d9aa7fb4167a412f4840301a780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/1641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Define the Supervised Fine-Tuning Trainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train, # Pass the dataset that has ALREADY been tokenized\n",
    "    eval_dataset=tokenized_test,   # It's good practice to include the evaluation set too\n",
    "    peft_config=lora_config,\n",
    "    args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0zxHVsWnAoTH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0zxHVsWnAoTH",
    "outputId": "24b296de-7d2b-4cec-c2ec-76c530fc3981"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18460' max='18460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18460/18460 1:55:18, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.952900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.752200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.705400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.665100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.655600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.645000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.624600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.605800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.586900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.584500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.559300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.565600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.530600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.555100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.544400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.534400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.512900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.507400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.508500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.498000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.496300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.503000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.496900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.477400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.500700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.478200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.491800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.467200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.472100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.467200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.469900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.472500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.463400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.451900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.466000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18460, training_loss=1.5458635588508516, metrics={'train_runtime': 6919.1469, 'train_samples_per_second': 21.341, 'train_steps_per_second': 2.668, 'total_flos': 2.5605556367616e+16, 'train_loss': 1.5458635588508516})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Train the model\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6354a641",
   "metadata": {
    "id": "6354a641"
   },
   "outputs": [],
   "source": [
    "# @title Save the Model\n",
    "\n",
    "trainer.save_model(\"./fine-tuned-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54e61cfe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54e61cfe",
    "outputId": "a4f5615d-4be2-48d0-dc4a-10c46fc8deca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-training response to 'What is hereditary xanthinuria ?':\n",
      "Answer: What is hereditary xanthinuria?\n",
      "Hereditary xanthinuria (HXA) is a disorder characterized by a deficiency of xanthine, the active form of the vitamin that is found in the body. These deficiencies are caused by the xanthine-rich body protein called xanthine inositol. Xanthine is a metabolite that is stored in the liver and binds to proteins to form the enzyme xanthine. The enzyme xanthine is important for normal blood flow and blood clotting.\n"
     ]
    }
   ],
   "source": [
    "# @title Test the model after training\n",
    "test_question_after_training = \"What is hereditary xanthinuria ?\" # You can change this question\n",
    "post_training_response = test_model_response(model, tokenizer, test_question_after_training)\n",
    "print(f\"Post-training response to '{test_question_after_training}':\")\n",
    "print(f\"Answer: {post_training_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "yFzBfitjdM40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "id": "yFzBfitjdM40",
    "outputId": "dba6b3d4-2906-486b-e04a-5167dd5ac781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model on test dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='206' max='206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [206/206 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "- eval_loss: 1.5418\n",
      "- eval_runtime: 40.3260\n",
      "- eval_samples_per_second: 40.6930\n",
      "- eval_steps_per_second: 5.1080\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Evaluate the model after training\n",
    "\n",
    "print(\"\\nEvaluating model on test dataset...\")\n",
    "\n",
    "# Create a sliced version of the test dataset\n",
    "# This will take the first 25 examples from your tokenized_test dataset\n",
    "#subset_test_dataset = tokenized_test.select(range(50))\n",
    "\n",
    "evaluation_results = trainer.evaluate(eval_dataset=tokenized_test)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "for key, value in evaluation_results.items():\n",
    "    print(f\"- {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
