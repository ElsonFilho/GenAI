{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhLicf96y1sA"
   },
   "source": [
    "# Implementing Tokenization\n",
    "\n",
    "Tokenizers play a pivotal role in natural language processing, segmenting text into smaller units known as tokens. These tokens are subsequently transformed into numerical representations called token indices, which are directly employed by deep learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qxuelFo7zWAq",
    "outputId": "3ba4563a-4e99-474a-9dd5-4882432dc060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Installing Libraries ---\n",
      "\n",
      "--- Downloading spaCy Models ---\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "\n",
      "--- Installation and Downloads Complete ---\n",
      "If prompted by spaCy, please RESTART THE RUNTIME (Runtime -> Restart runtime) now.\n",
      "This is crucial for spaCy models to load correctly.\n"
     ]
    }
   ],
   "source": [
    "# Install Necessary Libraries and Download Models\n",
    "\n",
    "print(\"--- Installing Libraries ---\")\n",
    "\n",
    "# Install NLTK (often used for basic NLP tasks)\n",
    "!pip install -qqq nltk\n",
    "\n",
    "# Install Hugging Face Transformers\n",
    "# Use -qqq for quiet output to keep the log clean\n",
    "!pip install -qqq transformers\n",
    "\n",
    "# Install SentencePiece for subword tokenization (often a dependency of models)\n",
    "!pip install -qqq sentencepiece\n",
    "\n",
    "# Install spaCy for advanced NLP tasks\n",
    "!pip install -qqq spacy\n",
    "\n",
    "# Download spaCy language models\n",
    "# These downloads are often prompted to require a runtime restart.\n",
    "print(\"\\n--- Downloading spaCy Models ---\")\n",
    "!python -m spacy download en_core_web_sm -qqq\n",
    "!python -m spacy download de_core_news_sm -qqq\n",
    "\n",
    "print(\"\\n--- Installation and Downloads Complete ---\")\n",
    "print(\"If prompted by spaCy, please RESTART THE RUNTIME (Runtime -> Restart runtime) now.\")\n",
    "print(\"This is crucial for spaCy models to load correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uoxGQZW6yxAg",
    "outputId": "6cab0779-edbb-4f3d-a374-123bf5271883"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ensuring NLTK 'punkt_tab' tokenizer is downloaded ---\n",
      "Downloading NLTK 'punkt_tab' tokenizer...\n",
      "NLTK 'punkt_tab' tokenizer downloaded successfully.\n",
      "\n",
      "All necessary libraries imported and NLTK data checked.\n",
      "spaCy 'en_core_web_sm' and 'de_core_news_sm' models loaded.\n"
     ]
    }
   ],
   "source": [
    "# @title 2. Import Libraries and NLTK Data (Run AFTER restarting session if prompted by Install Cell)\n",
    "# Run this cell AFTER you have restarted the session if the *installation cell* prompted you to.\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import spacy\n",
    "from transformers import AutoTokenizer # We'll use AutoTokenizer for transformers\n",
    "\n",
    "# --- Explicit NLTK Data Download ---\n",
    "# Ensure 'punkt_tab' is downloaded. This is the newer, required resource for NLTK tokenizers.\n",
    "print(\"--- Ensuring NLTK 'punkt_tab' tokenizer is downloaded ---\")\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab') # Check for punkt_tab specifically\n",
    "    print(\"NLTK 'punkt_tab' tokenizer already present.\")\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK 'punkt_tab' tokenizer...\")\n",
    "    nltk.download('punkt_tab', quiet=True) # Download punkt_tab\n",
    "    print(\"NLTK 'punkt_tab' tokenizer downloaded successfully.\")\n",
    "# --- End Explicit NLTK Data Download ---\n",
    "\n",
    "\n",
    "print(\"\\nAll necessary libraries imported and NLTK data checked.\")\n",
    "\n",
    "# Load spaCy models\n",
    "try:\n",
    "    nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "    nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "    print(\"spaCy 'en_core_web_sm' and 'de_core_news_sm' models loaded.\")\n",
    "except OSError:\n",
    "    print(\"Error loading spaCy models. Did you restart the session after downloading them?\")\n",
    "    print(\"Please restart the session (Runtime -> Restart session) and re-run this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8MalLD-OCq8M",
    "outputId": "9bd129a4-3431-4f0b-e2b6-38839d20589c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example texts defined.\n"
     ]
    }
   ],
   "source": [
    "# Example Text for Tokenization\n",
    "# A sample text to demonstrate different tokenization methods.\n",
    "\n",
    "english_text = \"Natural language processing (NLP) is a field of artificial intelligence. It focuses on the interaction between computers and human language.\"\n",
    "german_text = \"Natürliche Sprachverarbeitung (NLP) ist ein Bereich der künstlichen Intelligenz. Sie konzentriert sich auf die Interaktion zwischen Computern und menschlicher Sprache.\"\n",
    "\n",
    "print(\"Example texts defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 879,
     "referenced_widgets": [
      "a1159c52652d48a3843b92a6a66caf53",
      "56a88008bbcb4048af07703e3f44b98e",
      "3ac03d6c506445e9805c1698ebb0208c",
      "4216c06ca4204f848c0cc33f93b63c0b",
      "7b7c716082f94818b305a2c95bd5f26b",
      "e368cb66467443689d914add67046d53",
      "8cb2109fea0d49768eb9fb0527389bdb",
      "b799177c28804021b27a75ffe202dc67",
      "0d37fa6092004e8ead5d2b591150b008",
      "9759e10e27734be4a4008daa74e75ea9",
      "2aa7bd83b7da4b51b751a2660cbcf4df",
      "5495daaa36bd430cb5ed0a199aad9a6d",
      "5f3ef3dc8b9a4ad788a083857c2f9762",
      "0fa6fd6edbae4f76ba680d3e622011e4",
      "14e5e6024d8645b384cbd8381d1356ee",
      "695e5ca5a75b4c6fa30c147139c4ac5d",
      "1ab12b5d3c6a4a7ebe9747058698a289",
      "baf8431f0ded40428ec18aba6fcdb765",
      "70854ba9950b4de4bb2407d51a43dc33",
      "e2277b62819548c9a33b13cb2994e773",
      "1296fe7782c74b9aaf802b0f3b005a70",
      "3b783ebf3e454628b4ce52c62e836b7a",
      "4eda2b2c2ff448bcb20b712856c2316a",
      "a1dca8a2ffe44f4e8322d62dbd2028be",
      "8fe1713556bc4d92937eff0b84f6017b",
      "f6e63ecb95494cb8a2dd02b81e9b8e0b",
      "7ee2b9f6c7fe4d6c91f70a90e65f46ae",
      "6020e6ac41a14d0f8a5b89a5f1f4e482",
      "4146bd944d684281ba178508141117fa",
      "ee430386944346199b82146e909b9b85",
      "ef35bdd5691a4677ab8ea49e9afd8aaa",
      "99df58b7ca1641c797ef737e8afc90ed",
      "bc283d01f0f94f01a57767e24a03ead3",
      "913653b5221e41389f35af96f694d364",
      "5729f1e4de02480587114cc56748bd63",
      "f3c2019a02f945f7aea28200f4acfe73",
      "b3e72e47a12640c5ade8ad4bb5cbb81a",
      "0993efb8d3814591b606023deb264671",
      "a20c5cf411cc4a5daac77f77bdc31e53",
      "67d3e76929fa4b91b64cfa08d5e12a90",
      "a046f01cc03e431bbcbbccc32070f15f",
      "766263d2ba2143c59b2ede1abc005802",
      "691ba19ec70f4381aec6fbc19dd2e9d1",
      "0b77bb6a08b6486f8bcbf955f322d169",
      "e9aa65d4a6064b7eb7352898e7c01dbb",
      "89ff8b23693148a48c313147947541d3",
      "c57402f4378447429909c12cd8d96dfd",
      "1745676632a545b982a1de078a732216",
      "f9ea79b511d941ae97f8d1ab188e2b57",
      "e31b3af5d8954441bf2a608f94c85128",
      "89e4da92215d49d6b50b8aa1398e31c3",
      "b8c13dcb155a4426bce84142e0eea658",
      "669704d685314980a88c3710ae5c60ab",
      "895551c905ce4b229b6579e11e7902ae",
      "c3569b669613496491736cd91314d215",
      "8f1f71f5ea574b7ba3e783c06bad4b23",
      "22884336250d49cdbe4394039ff7b488",
      "c6f2e0a9828744e18b76a7b6a2570be4",
      "d71debc9e2ae4dc3a53a008179f9ff47",
      "f44b44e872524dcdb6ae27057045d901",
      "24fb5d488aba46fbbacbdb0dce53e8c9",
      "ed8ef3883e0b43b08b28be90123c0c9c",
      "9a01ce1b11fe4490a00e6c840e350fb2",
      "728298b541654129b648ae2eb51ef1f2",
      "21fe837cac0e45cf83f5e865f19eeca0",
      "73ef55bbed404f6db3ab0eb15949c94f",
      "176d0e85503f4fdba6935f7184681460",
      "2fa7e8a8f3434e39871c3043f4629ff2",
      "24ee9e06390847fa9029c13210342e93",
      "1f4bb19356a940f7b0c382d87ca95be2",
      "36ff63ecd68d418fbc7ce34a71f374f9",
      "50f5dcc5ac4f4fe3895e6442ad340277",
      "ef4245d8fe5c42c582ccfd95c1c05709",
      "03d7f952f9f646999f2556f41c22a576",
      "7e36c7810c6c4d47855a668ad94ef853",
      "634dffe2e3cc447a9a1a2c7ec3d10a2a",
      "075b5285694546ab997f1768248f341c",
      "96010f88b18e48d4ad62d55e5956f617",
      "8963c950d11248b8b3f4f14780bac13a",
      "1ef72bd0d5a74000af5933ad0581145c",
      "38df885de3c04309bd1ce06d63669dc0",
      "365aacf4e72c4222aeaad7b36b2c35d0",
      "089cb60ba8f4411cab6fddfadbe8a61f",
      "2679ff132bd142698b19b96992f4045d",
      "f4d8864b5e794d488ac4f96db11e85f4",
      "68b6bcc344aa4f04a30931b70803465d",
      "57da32d3e9a2494889652c2a0e6bce1c",
      "d9ae4305e5224523aae3b8239e9811dd"
     ]
    },
    "id": "3qkfOpPHC2vY",
    "outputId": "85dc362b-9747-4d4f-f53c-c422c9e5b64b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- NLTK Tokenization ---\n",
      "NLTK Word Tokens (English): ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of']...\n",
      "NLTK Sentence Tokens (English): ['Natural language processing (NLP) is a field of artificial intelligence.', 'It focuses on the interaction between computers and human language.']\n",
      "Character Tokens (English - manual): ['N', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'p', 'r', 'o']...\n",
      "\n",
      "--- spaCy Tokenization ---\n",
      "spaCy Word Tokens (English): ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of']...\n",
      "spaCy Sentence Tokens (English): ['Natural language processing (NLP) is a field of artificial intelligence.', 'It focuses on the interaction between computers and human language.']\n",
      "spaCy Word Tokens (German): ['Natürliche', 'Sprachverarbeitung', '(', 'NLP', ')', 'ist', 'ein', 'Bereich', 'der', 'künstlichen']...\n",
      "\n",
      "--- Hugging Face Transformers Tokenization (Subword) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1159c52652d48a3843b92a6a66caf53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5495daaa36bd430cb5ed0a199aad9a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eda2b2c2ff448bcb20b712856c2316a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913653b5221e41389f35af96f694d364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face (BERT) Subword Tokens (English): ['natural', 'language', 'processing', '(', 'nl', '##p', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', '.', 'it']...\n",
      "Hugging Face (BERT) Encoded Input (with special tokens):\n",
      "  Input IDs: tensor([  101,  3019,  2653,  6364,  1006, 17953,  2361,  1007,  2003,  1037,\n",
      "         2492,  1997,  7976,  4454,  1012,  2009,  7679,  2006,  1996,  8290,\n",
      "         2090,  7588,  1998,  2529,  2653,  1012,   102])\n",
      "  Decoded: [CLS] natural language processing ( nlp ) is a field of artificial intelligence. it focuses on the interaction between computers and human language. [SEP]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9aa65d4a6064b7eb7352898e7c01dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1f71f5ea574b7ba3e783c06bad4b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176d0e85503f4fdba6935f7184681460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96010f88b18e48d4ad62d55e5956f617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face (ALBERT) Subword Tokens (English): ['▁natural', '▁language', '▁processing', '▁', '(', 'n', 'lp', ')', '▁is', '▁a', '▁field', '▁of', '▁artificial', '▁intelligence', '.']...\n",
      "\n",
      "--- Comparison of Tokenization Methods ---\n",
      "Original English Text Length: 140 characters\n",
      "NLTK Word Tokens Count: 24\n",
      "Character Tokens Count: 140\n",
      "Hugging Face (BERT) Subword Tokens Count: 25\n",
      "\n",
      "Notice how subword tokenization often breaks down words like 'processing' into 'process' and '##ing'.\n",
      "This helps handle out-of-vocabulary words and reduces vocabulary size while retaining semantic information.\n"
     ]
    }
   ],
   "source": [
    "# Tokenization Examples\n",
    "\n",
    "print(\"--- NLTK Tokenization ---\")\n",
    "# NLTK: Word Tokenization\n",
    "nltk_words = word_tokenize(english_text)\n",
    "print(f\"NLTK Word Tokens (English): {nltk_words[:10]}...\") # Show first 10 for brevity\n",
    "\n",
    "# NLTK: Sentence Tokenization\n",
    "nltk_sentences = sent_tokenize(english_text)\n",
    "print(f\"NLTK Sentence Tokens (English): {nltk_sentences}\")\n",
    "\n",
    "# NLTK: Character-based (simple manual implementation for demonstration)\n",
    "# NLTK doesn't have a built-in character tokenizer as a primary function,\n",
    "# but it's easy to do manually.\n",
    "char_tokens_english = list(english_text)\n",
    "print(f\"Character Tokens (English - manual): {char_tokens_english[:20]}...\") # Show first 20\n",
    "\n",
    "\n",
    "print(\"\\n--- spaCy Tokenization ---\")\n",
    "# spaCy: Word and Sentence Tokenization (more advanced, handles punctuation, etc.)\n",
    "doc_en = nlp_en(english_text)\n",
    "spacy_words_en = [token.text for token in doc_en]\n",
    "print(f\"spaCy Word Tokens (English): {spacy_words_en[:10]}...\")\n",
    "\n",
    "spacy_sentences_en = [sent.text for sent in doc_en.sents]\n",
    "print(f\"spaCy Sentence Tokens (English): {spacy_sentences_en}\")\n",
    "\n",
    "# spaCy for German\n",
    "doc_de = nlp_de(german_text)\n",
    "spacy_words_de = [token.text for token in doc_de]\n",
    "print(f\"spaCy Word Tokens (German): {spacy_words_de[:10]}...\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Hugging Face Transformers Tokenization (Subword) ---\")\n",
    "# Hugging Face Transformers: Subword Tokenization (e.g., using BERT's tokenizer)\n",
    "# We'll use a pre-trained tokenizer, which typically uses a subword algorithm\n",
    "# like WordPiece (for BERT) or SentencePiece (for ALBERT, XLNet, etc.)\n",
    "\n",
    "# Load a tokenizer for a common model (e.g., 'bert-base-uncased')\n",
    "# 'uncased' means it converts text to lowercase before tokenizing\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "hf_tokens_bert = tokenizer_bert.tokenize(english_text)\n",
    "print(f\"Hugging Face (BERT) Subword Tokens (English): {hf_tokens_bert[:15]}...\") # Show first 15\n",
    "\n",
    "# Example with token IDs and special tokens\n",
    "encoded_input = tokenizer_bert(english_text, return_tensors='pt', add_special_tokens=True)\n",
    "print(f\"Hugging Face (BERT) Encoded Input (with special tokens):\")\n",
    "print(f\"  Input IDs: {encoded_input['input_ids'][0]}\")\n",
    "print(f\"  Decoded: {tokenizer_bert.decode(encoded_input['input_ids'][0])}\")\n",
    "\n",
    "# Another example with a different subword tokenizer (e.g., ALBERT uses SentencePiece)\n",
    "tokenizer_albert = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "hf_tokens_albert = tokenizer_albert.tokenize(english_text)\n",
    "print(f\"Hugging Face (ALBERT) Subword Tokens (English): {hf_tokens_albert[:15]}...\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Comparison of Tokenization Methods ---\")\n",
    "print(f\"Original English Text Length: {len(english_text)} characters\")\n",
    "print(f\"NLTK Word Tokens Count: {len(nltk_words)}\")\n",
    "print(f\"Character Tokens Count: {len(char_tokens_english)}\")\n",
    "print(f\"Hugging Face (BERT) Subword Tokens Count: {len(hf_tokens_bert)}\")\n",
    "\n",
    "print(\"\\nNotice how subword tokenization often breaks down words like 'processing' into 'process' and '##ing'.\")\n",
    "print(\"This helps handle out-of-vocabulary words and reduces vocabulary size while retaining semantic information.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
