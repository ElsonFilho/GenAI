{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2MUVbpEh5pv"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gcaqhohphvwl"
   },
   "source": [
    "We will nedd the following libraries:\n",
    "* numpy\n",
    "* pandas\n",
    "* matplotlib\n",
    "* pytorch\n",
    "* torchtext\n",
    "* transformers\n",
    "\n",
    "Let's first check what is installed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-KVuYmnqfl-o",
    "outputId": "03c1d7f3-252f-4006-e593-ead5788c6dbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy is installed. Version: 2.0.2\n",
      "------------------------------\n",
      "Pandas is installed. Version: 2.2.2\n",
      "------------------------------\n",
      "Matplotlib is installed. Version: 2.2.2\n",
      "------------------------------\n",
      "PyTorch is installed. Version: 2.6.0+cu124\n",
      "  CUDA is NOT available (as expected for CPU-only).\n",
      "  PyTorch device: cpu\n",
      "------------------------------\n",
      "TorchText (torchtext) is not installed.\n",
      "------------------------------\n",
      "Transformers is installed. Version: 4.53.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"NumPy is installed. Version: {np.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"NumPy is not installed.\")\n",
    "\n",
    "print(\"-\" * 30) # Just a separator for better readability\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(f\"Pandas is installed. Version: {pd.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Pandas is not installed.\")\n",
    "\n",
    "print(\"-\" * 30) # Just a separator for better readability\n",
    "\n",
    "try:\n",
    "    import matplotlib as plt\n",
    "    print(f\"Matplotlib is installed. Version: {pd.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Matplotlib is not installed.\")\n",
    "\n",
    "print(\"-\" * 30) # Just a separator for better readability\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch is installed. Version: {torch.__version__}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"  CUDA is available (though you mentioned CPU-only).\")\n",
    "    else:\n",
    "        print(\"  CUDA is NOT available (as expected for CPU-only).\")\n",
    "    print(f\"  PyTorch device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch (torch) is not installed.\")\n",
    "\n",
    "print(\"-\" * 30) # Just a separator for better readability\n",
    "try:\n",
    "    import torchtext\n",
    "    print(f\"TorchText is installed. Version: {torchtext.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"TorchText (torchtext) is not installed.\")\n",
    "\n",
    "print(\"-\" * 30) # Just a separator for better readability\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"Transformers is installed. Version: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Transformers (transformers) is not installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xpQYLsFZoDtK",
    "outputId": "e5cf4d83-c12c-49b2-e7b0-86b140b3f279"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 0: Initial environment check ---\n",
      "sentence-transformers                 4.1.0\n",
      "torch                                 2.3.1+cpu\n",
      "torchao                               0.10.0\n",
      "torchaudio                            2.3.1+cpu\n",
      "torchdata                             0.11.0\n",
      "torchsummary                          1.5.1\n",
      "torchtune                             0.6.1\n",
      "torchvision                           0.18.1+cpu\n",
      "\n",
      "--- Step 1: Aggressively uninstalling existing PyTorch, TorchVision, TorchAudio, and TorchText/Transformers ---\n",
      "Found existing installation: torch 2.3.1+cpu\n",
      "Uninstalling torch-2.3.1+cpu:\n",
      "  Successfully uninstalled torch-2.3.1+cpu\n",
      "Found existing installation: torchvision 0.18.1+cpu\n",
      "Uninstalling torchvision-0.18.1+cpu:\n",
      "  Successfully uninstalled torchvision-0.18.1+cpu\n",
      "Found existing installation: torchaudio 2.3.1+cpu\n",
      "Uninstalling torchaudio-2.3.1+cpu:\n",
      "  Successfully uninstalled torchaudio-2.3.1+cpu\n",
      "\u001b[33mWARNING: Skipping torchtext as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "--- Step 1.5: Verifying uninstallation ---\n",
      "sentence-transformers                 4.1.0\n",
      "torchao                               0.10.0\n",
      "torchdata                             0.11.0\n",
      "torchsummary                          1.5.1\n",
      "torchtune                             0.6.1\n",
      "\n",
      "--- Step 2: Installing PyTorch 2.3.1 (CPU-only) ---\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch==2.3.1+cpu\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torch-2.3.1%2Bcpu-cp311-cp311-linux_x86_64.whl (190.4 MB)\n",
      "Collecting torchvision==0.18.1+cpu\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torchvision-0.18.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.6 MB)\n",
      "Collecting torchaudio==2.3.1+cpu\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torchaudio-2.3.1%2Bcpu-cp311-cp311-linux_x86_64.whl (1.7 MB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cpu) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cpu) (4.14.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cpu) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cpu) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cpu) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cpu) (2025.3.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.18.1+cpu) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.18.1+cpu) (11.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.3.1+cpu) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.3.1+cpu) (1.3.0)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
      "peft 0.15.2 requires transformers, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.3.1+cpu torchaudio-2.3.1+cpu torchvision-0.18.1+cpu\n",
      "\n",
      "--- Step 3: Installing TorchText 0.18.0 (compatible with PyTorch 2.3.x) ---\n",
      "Collecting torchtext==0.18.0\n",
      "  Using cached torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.18.0) (4.67.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.18.0) (2.32.3)\n",
      "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from torchtext==0.18.0) (2.3.1+cpu)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.18.0) (2.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (4.14.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (2025.3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.18.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.18.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.18.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.18.0) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->torchtext==0.18.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2.3.0->torchtext==0.18.0) (1.3.0)\n",
      "Using cached torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.18.0\n",
      "\n",
      "--- Step 4: Installing Hugging Face Transformers ---\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
      "Downloading transformers-4.53.1-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "Successfully installed transformers-4.53.1\n",
      "\n",
      "--- Step 5: Final environment check before restart ---\n",
      "sentence-transformers                 4.1.0\n",
      "torch                                 2.3.1+cpu\n",
      "torchao                               0.10.0\n",
      "torchaudio                            2.3.1+cpu\n",
      "torchdata                             0.11.0\n",
      "torchsummary                          1.5.1\n",
      "torchtext                             0.18.0\n",
      "torchtune                             0.6.1\n",
      "torchvision                           0.18.1+cpu\n",
      "transformers                          4.53.1\n",
      "\n",
      "\n",
      "#################################################################################\n",
      "### IMPORTANT: PLEASE RESTART YOUR COLAB RUNTIME NOW (Runtime -> Restart runtime) ###\n",
      "### After restarting, run your import cell to verify.                       ###\n",
      "#################################################################################\n"
     ]
    }
   ],
   "source": [
    "# --- EXTREMELY IMPORTANT: Restart your Colab runtime AFTER running this entire cell ---\n",
    "\n",
    "print(\"--- Step 0: Initial environment check ---\")\n",
    "!pip list | grep -E \"torch|transformers\"\n",
    "\n",
    "# 1. AGGRESSIVE UNINSTALLATION (without --force-dep)\n",
    "#    We will simply list everything to uninstall.\n",
    "print(\"\\n--- Step 1: Aggressively uninstalling existing PyTorch, TorchVision, TorchAudio, and TorchText/Transformers ---\")\n",
    "!pip uninstall -y torch torchvision torchaudio torchtext transformers\n",
    "\n",
    "# Verify uninstallation\n",
    "print(\"\\n--- Step 1.5: Verifying uninstallation ---\")\n",
    "!pip list | grep -E \"torch|transformers\" # Should show nothing or very little\n",
    "\n",
    "# 2. Install PyTorch 2.3.1 (CPU-only).\n",
    "#    This version is compatible with TorchText 0.18.0.\n",
    "#    You can verify the exact command on https://pytorch.org/get-started/locally/ by selecting\n",
    "#    PyTorch Build: stable, Your OS: Linux, Package: Pip, Compute Platform: CPU, PyTorch version: 2.3.1\n",
    "print(\"\\n--- Step 2: Installing PyTorch 2.3.1 (CPU-only) ---\")\n",
    "!pip install torch==2.3.1+cpu torchvision==0.18.1+cpu torchaudio==2.3.1+cpu --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# 3. Install TorchText 0.18.0.\n",
    "#    This is the latest version that pip lists as available for your environment,\n",
    "#    and it is compatible with PyTorch 2.3.x.\n",
    "print(\"\\n--- Step 3: Installing TorchText 0.18.0 (compatible with PyTorch 2.3.x) ---\")\n",
    "!pip install torchtext==0.18.0\n",
    "\n",
    "# 4. Install Hugging Face Transformers.\n",
    "#    Transformers usually works well with recent PyTorch versions, including 2.3.x.\n",
    "print(\"\\n--- Step 4: Installing Hugging Face Transformers ---\")\n",
    "!pip install transformers\n",
    "\n",
    "print(\"\\n--- Step 5: Final environment check before restart ---\")\n",
    "!pip list | grep -E \"torch|transformers\"\n",
    "\n",
    "print(\"\\n\\n#################################################################################\")\n",
    "print(\"### IMPORTANT: PLEASE RESTART YOUR COLAB RUNTIME NOW (Runtime -> Restart runtime) ###\")\n",
    "print(\"### After restarting, run your import cell to verify.                       ###\")\n",
    "print(\"#################################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jo6j5EosjbHl",
    "outputId": "a6ddb4a7-e9d7-4a9a-db18-12e280d8430d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/usr/local/lib/python3.11/dist-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/usr/local/lib/python3.11/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/usr/local/lib/python3.11/dist-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from itertools import chain\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Transformer, CrossEntropyLoss\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "# Suppress warnings (keep at the bottom or in a separate utility cell)\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrnxHe2QpZ6I"
   },
   "source": [
    "DATA\n",
    "\n",
    "The CSV datafiles were uploaded in /content/sample_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nDWWCK-pt7H"
   },
   "outputs": [],
   "source": [
    "class BERTCSVDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        self.data = pd.read_csv(filename)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        try:\n",
    "\n",
    "            bert_input = torch.tensor(json.loads(row['BERT Input']), dtype=torch.long)\n",
    "            bert_label = torch.tensor(json.loads(row['BERT Label']), dtype=torch.long)\n",
    "            segment_label = torch.tensor([int(x) for x in row['Segment Label'].split(',')], dtype=torch.long)\n",
    "            is_next = torch.tensor(row['Is Next'], dtype=torch.long)\n",
    "            original_text = row['Original Text']  # If you want to use it\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON for row {idx}: {e}\")\n",
    "            print(\"BERT Input:\", row['BERT Input'])\n",
    "            print(\"BERT Label:\", row['BERT Label'])\n",
    "            # Handle the error, e.g., by skipping this row or using default values\n",
    "            return None  # or some default values\n",
    "\n",
    "            # Tokenizing the original text with BERT\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "            original_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoded_input['input_ids'].squeeze()\n",
    "        attention_mask = encoded_input['attention_mask'].squeeze()\n",
    "\n",
    "        return(bert_input, bert_label, segment_label, is_next, input_ids, attention_mask, original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v11uK4H5p05d"
   },
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "def collate_batch(batch):\n",
    "\n",
    "\n",
    "    bert_inputs_batch, bert_labels_batch, segment_labels_batch, is_nexts_batch,input_ids_batch,attention_mask_batch,original_text_battch = [], [], [], [],[],[],[]\n",
    "\n",
    "    for bert_input, bert_label, segment_label, is_next,input_ids,attention_mask,original_text in batch:\n",
    "        # Convert each sequence to a tensor and append to the respective list\n",
    "        bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
    "        bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
    "        segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
    "        is_nexts_batch.append(is_next)\n",
    "        input_ids_batch.append(input_ids)\n",
    "        attention_mask_batch.append(attention_mask)\n",
    "        original_text_battch.append(original_text)\n",
    "\n",
    "    # Pad the sequences in the batch\n",
    "    bert_inputs_final = pad_sequence(bert_inputs_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    bert_labels_final = pad_sequence(bert_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    segment_labels_final = pad_sequence(segment_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    is_nexts_batch = torch.tensor(is_nexts_batch, dtype=torch.long)\n",
    "\n",
    "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRiXpvbkp3Yu"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataset_path = '/content/sample_data/bert_train_data_sampled.csv'\n",
    "test_dataset_path = '/content/sample_data/bert_test_data_sampled.csv'\n",
    "\n",
    "train_dataset = BERTCSVDataset(train_dataset_path)\n",
    "test_dataset = BERTCSVDataset(test_dataset_path)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLIaukNBqf-M"
   },
   "source": [
    "## Model Creation\n",
    "In BERT, positional embedding, token embedding, and segment embedding are three types of embeddings used to represent the input tokens in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7YlNrn-sqmSt"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Define the PositionalEncoding class as a PyTorch module for adding positional information to token embeddings\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a positional encoding matrix as per the Transformer paper's formula\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor):\n",
    "        # Apply the positional encodings to the input token embeddings\n",
    "\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAoehxGnqoQS"
   },
   "outputs": [],
   "source": [
    "class BERTEmbedding (nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size ,dropout=0.1,train=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = TokenEmbedding( vocab_size,emb_size )\n",
    "        self.positional_encoding = PositionalEncoding(emb_size,dropout)\n",
    "        self.segment_embedding = nn.Embedding(3, emb_size)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, bert_inputs, segment_labels=False):\n",
    "        my_embeddings=self.token_embedding(bert_inputs)\n",
    "        if self.train:\n",
    "          x = self.dropout(my_embeddings + self.positional_encoding(my_embeddings) + self.segment_embedding(segment_labels))\n",
    "        else:\n",
    "          x = my_embeddings + self.positional_encoding(my_embeddings)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Uk_S4ZQqv8h"
   },
   "source": [
    "Define a complete BERT model with the following key components:\n",
    "\n",
    "1. Initialization: The `BERT` class is defined as a subclass of `torch.nn.Module`. It initializes the BERT model with parameters such as vocabulary size, model dimension, number of layers, number of attention heads, and dropout rate.\n",
    "\n",
    "2. Embedding Layer: The BERT model includes an embedding layer that combines token embeddings and segment embeddings using the `BERTEmbedding` class.\n",
    "\n",
    "3. Transformer Encoder: Transformer Encoder layers are used to encode the input embeddings. The number of layers, attention heads, dropout rate, and model dimension are specified based on the defined parameters.\n",
    "\n",
    "4. Next Sentence Prediction: The model has a linear layer for Next Sentence Prediction. It takes the output from the Transformer encoder and predicts the relationship between two consecutive sentences, classifying them into two classes.\n",
    "\n",
    "5. Masked Language Modeling: The model also includes a linear layer for Masked Language Modeling. It predicts the masked tokens in the input sequence by taking the output from the Transformer encoder and making predictions across the vocabulary.\n",
    "\n",
    "6. Forward Pass: The `forward` method defines the forward pass of the BERT model. It takes input tokens (`bert_inputs`) and segment labels (`segment_labels`) and returns predictions for Next Sentence Prediction and Masked Language Modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMI1PdlIqrHV"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE=147161\n",
    "batch = 2\n",
    "count = 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# load sample batches from dataloader\n",
    "for batch in train_dataloader:\n",
    "    bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6vIg6GUrBJp",
    "outputId": "707686c2-e2b1-4a4b-d812-f37e870ebdfe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uG9B0UmSrFtG",
    "outputId": "59997eac-7d5b-487b-ea6d-3507fcf5b98b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1, 3033,   15, 1940,   23,  660, 2517,    3,    2,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    3,\n",
       "        7216,    7,    5,  101, 5794,  411,   31,  293,  135,    7,    3,    3,\n",
       "         187,   10,  426, 3423,  165,    7,   15,    9,    3,   10,   61,   29,\n",
       "        1550,    3,   73, 6537,  411,    3,    3, 2280,   58,    2,    0,    0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pick a sample input\n",
    "bert_inputs[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GaK0fITHrMkv",
    "outputId": "2d50f23b-d9b6-4233-e511-4c274f50b15b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([72, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yKsNj0AmrTN8",
    "outputId": "7ae5b293-8fe7-47f5-a60f-0617eb024e07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_labels[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AyBpKdU9rkyz",
    "outputId": "d3a80831-031d-4d0a-cf2b-c09b2696ebd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of token embeddings: torch.Size([72, 2, 10])\n",
      "Token Embeddings for the 0th token of the first sample: tensor([ 1.5384, -8.1250, -1.5859, -1.5011,  0.9171, -0.2698, -2.4455, -0.3522,\n",
      "        -1.1115,  1.9573], grad_fn=<SliceBackward0>)\n",
      "Token Embeddings for the 1th token of the first sample: tensor([ 2.6097,  2.0662, -5.2810, -0.1401, -0.4262,  2.7150, -3.9778, -2.1307,\n",
      "         0.1395, -1.3792], grad_fn=<SliceBackward0>)\n",
      "Token Embeddings for the 2th token of the first sample: tensor([ 4.7348,  4.7619,  8.4431,  1.3590,  3.0379, -2.8740,  5.4607, -3.8453,\n",
      "        -7.5723,  5.5518], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the TokenEmbedding\n",
    "token_embedding = TokenEmbedding(VOCAB_SIZE, emb_size=EMBEDDING_DIM )\n",
    "\n",
    "# Get the token embeddings for a sample input\n",
    "t_embeddings = token_embedding(bert_inputs)\n",
    "#Each token is transformed into a tensor of size emb_size\n",
    "print(f\"Dimensions of token embeddings: {t_embeddings.size()}\") # Expected: (sequence_length, batch_size, EMBEDDING_DIM)\n",
    "#Check the embedded vectors for first 3 tokens of the first sample in the batch\n",
    "# you get embeddings[i,0,:] where i refers to the i'th token of the first sample in the batch (b=0)\n",
    "for i in range(3):\n",
    "    print(f\"Token Embeddings for the {i}th token of the first sample: {t_embeddings[i,0,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfk0xescrn9a",
    "outputId": "9a4cc8b5-5a9a-4fb4-d925-859cd9e0a1cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of positionally encoded tokens: torch.Size([72, 2, 10])\n",
      "Positional Embeddings for the 0th token of the first sample: tensor([ 1.5384, -7.1250, -1.5859, -0.5011,  0.9171,  0.7302, -2.4455,  0.6478,\n",
      "        -1.1115,  2.9573], grad_fn=<SliceBackward0>)\n",
      "Positional Embeddings for the 1th token of the first sample: tensor([ 3.4512,  2.6065, -5.1232,  0.8474, -0.4011,  3.7147, -3.9738, -1.1307,\n",
      "         0.1401, -0.3792], grad_fn=<SliceBackward0>)\n",
      "Positional Embeddings for the 2th token of the first sample: tensor([ 5.6441,  4.3457,  8.7548,  2.3091,  3.0881, -1.8752,  5.4687, -2.8453,\n",
      "        -7.5711,  6.5518], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "positional_encoding = PositionalEncoding(emb_size=EMBEDDING_DIM,dropout=0)\n",
    "\n",
    "# Apply positional encoding to token embeddings\n",
    "p_embedding = positional_encoding(t_embeddings)\n",
    "\n",
    "print(f\"Dimensions of positionally encoded tokens: {p_embedding.size()}\")# Expected: (sequence_length, batch_size, EMBEDDING_DIM)\n",
    "#Check the positional encoded vectors for first 3 tokens of the first sample in the batch\n",
    "# you get encoded_tokens[i,0,:] where i refers to the i'th token of the first sample(b=0) in the batch\n",
    "for i in range(3):\n",
    "    print(f\"Positional Embeddings for the {i}th token of the first sample: {p_embedding[i,0,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmu8YgnVrsYb",
    "outputId": "ade8858d-1eac-47b4-ea7e-b8d8fdfa360c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of segment embedding: torch.Size([72, 2, 10])\n",
      "Segment Embeddings for the 0th token of the first sample: tensor([-0.6478, -0.2803, -0.9814, -0.6833, -0.3031,  1.5178, -1.7818,  0.6299,\n",
      "         0.6027, -0.4947], grad_fn=<SliceBackward0>)\n",
      "Segment Embeddings for the 1th token of the first sample: tensor([-0.6478, -0.2803, -0.9814, -0.6833, -0.3031,  1.5178, -1.7818,  0.6299,\n",
      "         0.6027, -0.4947], grad_fn=<SliceBackward0>)\n",
      "Segment Embeddings for the 2th token of the first sample: tensor([-0.6478, -0.2803, -0.9814, -0.6833, -0.3031,  1.5178, -1.7818,  0.6299,\n",
      "         0.6027, -0.4947], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "segment_embedding = nn.Embedding(3, EMBEDDING_DIM)\n",
    "s_embedding = segment_embedding(segment_labels)\n",
    "print(f\"Dimensions of segment embedding: {s_embedding.size()}\")# Expected: (sequence_length, batch_size, EMBEDDING_DIM)\n",
    "#Check the Segment Embedding vectors for first 3 tokens of the first sample in the batch\n",
    "# you get segment_embedded[i,0,:] where i refers to the i'th token of the first sample(b=0) in the batch\n",
    "for i in range(3):\n",
    "    print(f\"Segment Embeddings for the {i}th token of the first sample: {s_embedding[i,0,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O1wq9jXKrvLU",
    "outputId": "e226b601-558e-4363-a7dd-c4980c386666"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of token + position + segment encoded tokens: torch.Size([72, 2, 10])\n",
      "BERT_Embedding for 0th token: tensor([  2.4290, -15.5302,  -4.1533,  -2.6855,   1.5310,   1.9782,  -6.6728,\n",
      "          0.9254,  -1.6202,   4.4199], grad_fn=<SliceBackward0>)\n",
      "BERT_Embedding for 1th token: tensor([  5.4131,   4.3923, -11.3857,   0.0240,  -1.1305,   7.9475,  -9.7333,\n",
      "         -2.6316,   0.8824,  -2.2530], grad_fn=<SliceBackward0>)\n",
      "BERT_Embedding for 2th token: tensor([  9.7311,   8.8273,  16.2165,   2.9848,   5.8229,  -3.2314,   9.1476,\n",
      "         -6.0607, -14.5406,  11.6088], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Create the combined embedding vectors\n",
    "bert_embeddings = t_embeddings + p_embedding + s_embedding\n",
    "print(f\"Dimensions of token + position + segment encoded tokens: {bert_embeddings.size()}\")\n",
    "#Check the BERT Embedding vectors for first 3 tokens of the first sample in the batch\n",
    "# you get bert_embeddings[i,0,:] where i refers to the i'th token of the first sample(b=0) in the batch\n",
    "for i in range(3):\n",
    "    print(f\"BERT_Embedding for {i}th token: {bert_embeddings[i,0,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Q4MyKLprzba"
   },
   "outputs": [],
   "source": [
    "class BERT(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):\n",
    "        \"\"\"\n",
    "        vocab_size: The size of the vocabulary.\n",
    "        d_model: The size of the embeddings (hidden size).\n",
    "        n_layers: The number of Transformer layers.\n",
    "        heads: The number of attention heads in each Transformer layer.\n",
    "        dropout: The dropout rate applied to embeddings and Transformer layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.heads = heads\n",
    "\n",
    "        # Embedding layer that combines token embeddings and segment embeddings\n",
    "        self.bert_embedding = BERTEmbedding(vocab_size, d_model, dropout)\n",
    "\n",
    "        # Transformer Encoder layers\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=heads, dropout=dropout,batch_first=False)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # Linear layer for Next Sentence Prediction\n",
    "        self.nextsentenceprediction = nn.Linear(d_model, 2)\n",
    "\n",
    "        # Linear layer for Masked Language Modeling\n",
    "        self.masked_language = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, bert_inputs, segment_labels):\n",
    "        \"\"\"\n",
    "        bert_inputs: Input tokens.\n",
    "        segment_labels: Segment IDs for distinguishing different segments in the input.\n",
    "        mask: Attention mask to prevent attention to padding tokens.\n",
    "\n",
    "        return: Predictions for next sentence task and masked language modeling task.\n",
    "        \"\"\"\n",
    "\n",
    "        padding_mask = (bert_inputs == PAD_IDX).transpose(0, 1)\n",
    "        # Generate embeddings from input tokens and segment labels\n",
    "        my_bert_embedding = self.bert_embedding(bert_inputs, segment_labels)\n",
    "\n",
    "        # Pass embeddings through the Transformer encoder\n",
    "        transformer_encoder_output = self.transformer_encoder(my_bert_embedding,src_key_padding_mask=padding_mask)\n",
    "\n",
    "\n",
    "        next_sentence_prediction = self.nextsentenceprediction(transformer_encoder_output[ 0,:])\n",
    "\n",
    "\n",
    "        # Masked Language Modeling: Predict all tokens in the sequence\n",
    "        masked_language = self.masked_language(transformer_encoder_output)\n",
    "\n",
    "        return  next_sentence_prediction, masked_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sp94X3a5r3DD"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = 147161  # Replace VOCAB_SIZE with your vocabulary size\n",
    "d_model = EMBEDDING_DIM  # Replace EMBEDDING_DIM with your embedding dimension\n",
    "n_layers = 2  # Number of Transformer layers\n",
    "initial_heads = 12 # Initial number of attention heads\n",
    "initial_heads = 2\n",
    "# Ensure the number of heads is a factor of the embedding dimension\n",
    "heads = initial_heads - d_model % initial_heads\n",
    "\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "# Create an instance of the BERT model\n",
    "model = BERT(vocab_size, d_model, n_layers, heads, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVclYbeFr8OI",
    "outputId": "6c75df0a-1213-4bf9-834e-d95189e3c74c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 72])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask = (bert_inputs == PAD_IDX).transpose(0, 1)\n",
    "padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uZM_AUPkr_Et",
    "outputId": "eec96a00-9ae2-41f1-fb4b-163131b24f94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([72, 2, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=heads, dropout=dropout,batch_first=False)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "# Pass embeddings through the Transformer encoder\n",
    "transformer_encoder_output = transformer_encoder(bert_embeddings,src_key_padding_mask=padding_mask)\n",
    "transformer_encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5I-YNHnsDOA",
    "outputId": "6ccad397-f6da-4f70-9f81-03e3ce5ebb1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSP Output Shape: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "nextsentenceprediction = nn.Linear(d_model, 2)\n",
    "nsp = nextsentenceprediction(transformer_encoder_output[ 0,:])\n",
    "#logits for NSP task\n",
    "print(f\"NSP Output Shape: {nsp.shape}\")  # Expected shape: (batch_size, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3HKkmTqwsGSh",
    "outputId": "12fc0b01-93e7-4ab6-ff14-66abf69fd37f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLM Output Shape: torch.Size([72, 2, 147161])\n"
     ]
    }
   ],
   "source": [
    "masked_language = nn.Linear(d_model, vocab_size)\n",
    "# Masked Language Modeling: Predict all tokens in the sequence\n",
    "mlm = masked_language(transformer_encoder_output)\n",
    "#logits for MLM task\n",
    "print(f\"MLM Output Shape: {mlm.shape}\")  # Expected shape: (seq_length, batch_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgsSk9IxsL6i"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "After creating the BERT model, the next step is training and evaluating its performance. To facilitate this, an `evaluate` function is defined with the following steps:\n",
    "\n",
    "1. Loss Function: The CrossEntropyLoss function is defined to calculate the loss between predicted and actual values.\n",
    "\n",
    "2. Function Arguments: The function takes arguments including the dataloader, model, loss function, and device.\n",
    "\n",
    "3. Evaluation Mode: The BERT model is put into evaluation mode using `model.eval()`, disabling dropout and training-specific behaviors. Variables are initialized to track the total loss, total next sentence loss, total mask loss, and total number of batches.\n",
    "\n",
    "4. Evaluation Loop: The function iterates through the batches in the provided dataloader.\n",
    "\n",
    "5. Forward Pass: A forward pass is performed with the BERT model to obtain predictions for the next sentence and masked language tasks.\n",
    "\n",
    "6. Loss Calculation: The losses for the next sentence and masked language tasks are calculated, and then summed up to obtain the total loss for the batch.\n",
    "\n",
    "7. Average Loss Calculation: The average loss, average next sentence loss, and average mask loss are calculated by dividing the total losses by the total number of batches.\n",
    "\n",
    "\n",
    "The `evaluate` function is used not only for evaluating the BERT model's performance but also during the training phase to assess the model's progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpt_d5cAsNph"
   },
   "outputs": [],
   "source": [
    "PAD_IDX=0\n",
    "loss_fn_mlm = nn.CrossEntropyLoss(ignore_index=PAD_IDX)# The loss function must ignore PAD tokens and only calculates loss for the masked tokens\n",
    "loss_fn_nsp = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MjB2J_3KsSsm",
    "outputId": "fb44f39d-8778-4959-bf41-9c73fe83c26e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVlcLB14sVs4"
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader=test_dataloader, model=model, loss_fn_mlm=loss_fn_mlm, loss_fn_nsp=loss_fn_nsp, device=device):\n",
    "    model.eval()  # Turn off dropout and other training-specific behaviors\n",
    "\n",
    "    total_loss = 0\n",
    "    total_next_sentence_loss = 0\n",
    "    total_mask_loss = 0\n",
    "    total_batches = 0\n",
    "    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "        for batch in dataloader:\n",
    "            bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
    "\n",
    "            # Forward pass\n",
    "            next_sentence_prediction, masked_language = model(bert_inputs, segment_labels)\n",
    "\n",
    "            # Calculate loss for next sentence prediction\n",
    "            # Ensure is_nexts is of the correct shape for CrossEntropyLoss\n",
    "            next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts.view(-1))\n",
    "\n",
    "            # Calculate loss for predicting masked tokens\n",
    "            # Flatten both masked_language predictions and bert_labels to match CrossEntropyLoss input requirements\n",
    "            mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))\n",
    "\n",
    "            # Sum up the two losses\n",
    "            loss = next_loss + mask_loss\n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "            else:\n",
    "                total_loss += loss.item()\n",
    "                total_next_sentence_loss += next_loss.item()\n",
    "                total_mask_loss += mask_loss.item()\n",
    "                total_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / (total_batches + 1)\n",
    "    avg_next_sentence_loss = total_next_sentence_loss / (total_batches + 1)\n",
    "    avg_mask_loss = total_mask_loss / (total_batches + 1)\n",
    "\n",
    "    print(f\"Average Loss: {avg_loss:.4f}, Average Next Sentence Loss: {avg_next_sentence_loss:.4f}, Average Mask Loss: {avg_mask_loss:.4f}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdIUklWXst7T"
   },
   "source": [
    "## Training\n",
    "The training process for the BERT model involves the following steps:\n",
    "\n",
    "1. Optimizer Definition: Before training starts, an optimizer is defined for training the BERT model. In this case, the Adam optimizer is used.\n",
    "\n",
    "2. Training Loop: Within each epoch, the training data is iterated through in batches.\n",
    "\n",
    "3. Forward Pass: For each batch, a forward pass is performed, where the BERT model predicts the next sentence and masked language tasks.\n",
    "\n",
    "4. Loss Calculation and Parameter Update: The loss is calculated based on the predicted and actual values. The model's parameters are then updated through backpropagation and gradient clipping.\n",
    "\n",
    "5. Epoch Evaluation: After each epoch, the average training loss is printed. The model's performance on the test set is evaluated. Additionally, the model is saved after each epoch.\n",
    "\n",
    "These steps are repeated for multiple epochs to train the BERT model and monitor its progress over time.\n",
    "\n",
    "**NOTE: The current DataLoader is quite huge, and it will take several hours for the model to train with such a huge dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "01WiL7Ravon4",
    "outputId": "3a408dae-f417-49c9-e184-9fe5343f695b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1, Step 500/5000: Loss: 12.5769, LR: 0.000100\n",
      "  Epoch 1, Step 1000/5000: Loss: 12.4643, LR: 0.000089\n",
      "  Epoch 1, Step 1500/5000: Loss: 12.4313, LR: 0.000078\n",
      "  Epoch 1, Step 2000/5000: Loss: 12.0297, LR: 0.000067\n",
      "  Epoch 1, Step 2500/5000: Loss: 12.6387, LR: 0.000056\n",
      "  Epoch 1, Step 3000/5000: Loss: 12.2778, LR: 0.000044\n",
      "  Epoch 1, Step 3500/5000: Loss: 12.1270, LR: 0.000033\n",
      "  Epoch 1, Step 4000/5000: Loss: 12.0945, LR: 0.000022\n",
      "  Epoch 1, Step 4500/5000: Loss: 12.3130, LR: 0.000011\n",
      "  Epoch 1, Step 5000/5000: Loss: 12.3767, LR: 0.000000\n",
      "Epoch 1 - Average training loss: 12.3292 | Epoch Duration: 0 days 00:38:40.666896\n",
      "Starting evaluation for Epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 1/1 [46:59<00:00, 2819.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 12.7899, Average Next Sentence Loss: 0.7254, Average Mask Loss: 12.0645\n",
      "Epoch 1 - Average evaluation loss: 12.7899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "\n",
    "# Training loop setup\n",
    "num_epochs = 1\n",
    "total_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "# Define the number of warmup steps, e.g., 10% of total\n",
    "warmup_steps = int(total_steps * 0.1)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                             num_warmup_steps=warmup_steps,\n",
    "                                             num_training_steps=total_steps)\n",
    "\n",
    "# Lists to store losses for plotting\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "# Calculate the interval for printing (e.g., every 10% of steps)\n",
    "print_interval = len(train_dataloader) // 10\n",
    "if print_interval == 0: # Ensure at least one print if dataloader is very small\n",
    "    print_interval = 1\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    epoch_start_time = pd.Timestamp.now() # To track epoch duration\n",
    "\n",
    "    # The key change is here: `disable=True` for the INNER tqdm\n",
    "    # This will prevent the step-by-step progress bar output\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1} Batch\", disable=True)):\n",
    "        # Move batch to device\n",
    "        bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        next_sentence_prediction, masked_language = model(bert_inputs, segment_labels)\n",
    "\n",
    "        # Calculate losses\n",
    "        next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts)\n",
    "        mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))\n",
    "        loss = next_loss + mask_loss\n",
    "\n",
    "        # Accumulate loss\n",
    "        current_loss_item = loss.item()\n",
    "        if not torch.isnan(loss): # Only add if not NaN\n",
    "            total_loss += current_loss_item\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update the learning rate\n",
    "\n",
    "        # Print update every 10% of steps\n",
    "        if (step + 1) % print_interval == 0 or (step + 1) == len(train_dataloader):\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            # Use `\\r` to overwrite the current line for frequent updates within one line,\n",
    "            # or just `print` for new lines. For 10% updates, `print` is fine.\n",
    "            print(f\"  Epoch {epoch + 1}, Step {step + 1}/{len(train_dataloader)}: \"\n",
    "                  f\"Loss: {current_loss_item:.4f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    epoch_end_time = pd.Timestamp.now()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch {epoch+1} - Average training loss: {avg_train_loss:.4f} \"\n",
    "          f\"| Epoch Duration: {epoch_duration}\")\n",
    "\n",
    "    # Evaluation after each epoch\n",
    "    print(f\"Starting evaluation for Epoch {epoch+1}...\")\n",
    "    eval_loss = evaluate(test_dataloader, model, loss_fn_nsp, loss_fn_mlm, device)\n",
    "    eval_losses.append(eval_loss)\n",
    "    print(f\"Epoch {epoch+1} - Average evaluation loss: {eval_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "DyMwsn1B-dTj",
    "outputId": "4b67eff4-0d41-4133-c1ea-02311cf5ee3a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the loss values\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(range(1,num_epochs+1), train_losses, label=\"Training Loss\", color='blue')\n",
    "plt.scatter(range(1,num_epochs+1), eval_losses, label=\"Evaluation Loss\", color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Evaluation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YS3IRL3w-iSW"
   },
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IaIa2kE4-gYE",
    "outputId": "108dfa93-000c-4351-f692-511ce50ce64a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second sentence follows the first\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer with the BERT model's vocabulary\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "def predict_nsp(sentence1, sentence2, model, tokenizer):\n",
    "    # Tokenize sentences with special tokens\n",
    "    tokens = tokenizer.encode_plus(sentence1, sentence2, return_tensors=\"pt\")\n",
    "    tokens_tensor = tokens[\"input_ids\"].to(device)\n",
    "    segment_tensor = tokens[\"token_type_ids\"].to(device)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        # Assuming the model returns NSP predictions first\n",
    "        nsp_prediction, _ = model(tokens_tensor, segment_tensor)\n",
    "        # Select the first element (first sequence) of the logits tensor\n",
    "        first_logits = nsp_prediction[0].unsqueeze(0)  # Adds an extra dimension, making it [1, 2]\n",
    "        logits = torch.softmax(first_logits, dim=1)\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Interpret the prediction\n",
    "    return \"Second sentence follows the first\" if prediction == 1 else \"Second sentence does not follow the first\"\n",
    "\n",
    "# Example usage\n",
    "sentence1 = \"The cat sat on the mat.\"\n",
    "sentence2 = \"It was a sunny day\"\n",
    "\n",
    "print(predict_nsp(sentence1, sentence2, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iYzxxzc_tvU"
   },
   "source": [
    "## Next Sentence Prediction (NSP) with BERT\n",
    "\n",
    "1. **Load the BERT pretrained model**: Import `BertForPreTraining` and `BertTokenizer` from `transformers`, and load the 'bert-base-uncased' pretrained model and tokenizer.\n",
    "2. **Prepare text input**: Encode a pair of sentences using the loaded tokenizer.\n",
    "3. **Perform NSP**: Pass the encoded input through the model and interpret the `seq_relationship_logits` to determine if the model predicts the sentences as consecutive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WoyejuQ3-zUk",
    "outputId": "71bc1da8-5d54-40a7-c5ab-9409cb1e52b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat sat on the [unused5].\n"
     ]
    }
   ],
   "source": [
    "def predict_mlm(sentence, model, tokenizer):\n",
    "    # Tokenize the input sentence and convert to token IDs, including special tokens\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    tokens_tensor = inputs.input_ids\n",
    "\n",
    "    # Create dummy segment labels filled with zeros, assuming it's needed by your model\n",
    "    segment_labels = torch.zeros_like(tokens_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model, now correctly handling the output tuple\n",
    "        output_tuple = model(tokens_tensor, segment_labels)\n",
    "\n",
    "        # Assuming the second element of the tuple contains the MLM logits\n",
    "        predictions = output_tuple[1]  # Adjusted based on your model's output\n",
    "\n",
    "        # Identify the position of the [MASK] token\n",
    "        mask_token_index = (tokens_tensor == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Get the predicted index for the [MASK] token from the MLM logits\n",
    "        predicted_index = torch.argmax(predictions[0, mask_token_index.item(), :], dim=-1)\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index.item()])[0]\n",
    "\n",
    "        # Replace [MASK] in the original sentence with the predicted token\n",
    "        predicted_sentence = sentence.replace(tokenizer.mask_token, predicted_token, 1)\n",
    "\n",
    "    return predicted_sentence\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sentence = \"The cat sat on the [MASK].\"\n",
    "print(predict_mlm(sentence, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGA0Nfdl_naj"
   },
   "source": [
    "## Masked Language Modeling (MLM) with BERT\n",
    "1. **Initialize the model and tokenizer**:\n",
    "   Load `BertForPreTraining` and `BertTokenizer` from the `transformers` library using the 'bert-base-uncased' model.\n",
    "2. **Prepare the masked sentence**:\n",
    "   Write a sentence and replace one word with `[MASK]`. For example, \"The capital of France is [MASK].\"\n",
    "3. **Tokenize and predict**:\n",
    "   Tokenize the masked sentence with `BertTokenizer.` Then, input it to `BertForPreTraining` and use the `prediction_logits` to find the most probable token that fits the mask.\n",
    "4. **Display the prediction**:\n",
    "   Convert the predicted token ID back to a token string and print out the predicted word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "80854db2beca4300a570a412bdf7369e",
      "e144908f7e674102b8bb78393ca84097",
      "092dd5d439154b2e90c658694fa97396",
      "90fddb9f6348485b9f1ab903e1e24b83",
      "280221d7085646dc9f85fdd7d0724051",
      "1a673c9059cb42fab588181cd70b5304",
      "4029f584319d424289051a24615660e8",
      "41c00230d6284d87af723d85f55a1ce5",
      "ed51224054a344b68f412fd9eb7690f4",
      "9551f570dca54898816c56290bf47cae",
      "ced62140907e404ca5e447e658ac293c"
     ]
    },
    "id": "PYd0nlJq-9YY",
    "outputId": "fe2a2152-a732-4781-a556-e16f800cebb4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80854db2beca4300a570a412bdf7369e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model thinks these sentences are NOT consecutive.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForPreTraining, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pretrained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pretrained model (weights)\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "# Prepare text pair for NSP\n",
    "text_1 = \"The cat sat on the mat\"\n",
    "text_2 = \"It was a sunny day\"\n",
    "# Encode text\n",
    "inputs = tokenizer(text_1, text_2, return_tensors=\"pt\")\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, next_sentence_label=torch.LongTensor([1]))\n",
    "    nsp_logits = outputs.seq_relationship_logits\n",
    "\n",
    "# Interpret the result for NSP\n",
    "if torch.argmax(nsp_logits, dim=-1).item() == 0:\n",
    "    print(\"The model thinks these sentences are NOT consecutive.\")\n",
    "else:\n",
    "    print(\"The model thinks these sentences are consecutive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NREmq6ZN_aQN",
    "outputId": "62450405-9eda-410e-e21c-4353b0a95ae4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: ['paris']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForPreTraining, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pretrained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pretrained model (weights)\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Prepare text with masked token\n",
    "masked_text = \"The capital of France is [MASK].\"\n",
    "# Tokenize and prepare for the model: Convert to tokens and add special tokens\n",
    "input_ids = tokenizer(masked_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids)\n",
    "    predictions = outputs.prediction_logits\n",
    "\n",
    "# Confirm we were able to predict 'Paris' as the masked token\n",
    "predicted_index = torch.argmax(predictions[0, input_ids[0] == tokenizer.mask_token_id]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
    "\n",
    "print(f\"Predicted token: {predicted_token}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
